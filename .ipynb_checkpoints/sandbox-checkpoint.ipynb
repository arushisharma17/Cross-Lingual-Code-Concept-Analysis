{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "905d5149-4a48-4e1b-a09a-3d8285aaba01",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] encoder filtered file is saved as: input.in.tok.sent_len\n",
      "[INFO] decoder filtered file is saved as: label.out.tok.sent_len\n",
      "[INFO] Number of skipped lines: 0\n",
      "Reading file:  input.in.tok.sent_len\n",
      "Saving output file\n",
      "######### Singletons #############\n",
      "['I', 'like', 'apples.', 'She', 'drives', 'car.', 'The', 'sun', 'is', 'bright.', 'He', 'reads', 'book.', 'They', 'live', 'in', 'house.']\n",
      "#####################################\n",
      "Types in vocab:  18\n",
      "Tokens in vocab:  20\n",
      "Types less than 2:  17\n",
      "Types less than 3:  17\n",
      "Types less than 4:  18\n",
      "Types less than 5:  18\n",
      "Reading file:  label.out.tok.sent_len\n",
      "Saving output file\n",
      "######### Singletons #############\n",
      "['Ich', 'mag', 'Äpfel.', 'fährt', 'Auto.', 'Die', 'Sonne', 'ist', 'hell.', 'Er', 'liest', 'Buch.', 'wohnen', 'in', 'einem', 'Haus.']\n",
      "#####################################\n",
      "Types in vocab:  18\n",
      "Tokens in vocab:  20\n",
      "Types less than 2:  16\n",
      "Types less than 3:  18\n",
      "Types less than 4:  18\n",
      "Types less than 5:  18\n",
      "Loading model: Salesforce/codet5-base,Salesforce/codet5-base,T5ForConditionalGeneration\n",
      "Reading input corpus\n",
      "Preparing output files\n",
      "Extracting representations from model\n",
      "(encoder) Sentence         : \"I like apples.\"\n",
      "(encoder) Original    (003): ['I', 'like', 'apples.']\n",
      "(encoder) Tokenized   (007): ['<s>', 'I', 'Ġlike', 'Ġap', 'ples', '.', '</s>']\n",
      "(decoder) Sentence         : \"Ich mag Äpfel.\"\n",
      "(decoder) Original    (003): ['Ich', 'mag', 'Äpfel.']\n",
      "(decoder) Tokenized   (011): ['<s>', 'I', 'ch', 'Ġmag', 'Ġ', 'Ã', 'Ħ', 'pf', 'el', '.', '</s>']\n",
      "Filtered   (005): ['I', 'Ġlike', 'Ġap', 'ples', '.']\n",
      "(encoder) Detokenized (003): ['I', 'Ġlike', 'Ġapples.']\n",
      "(encoder) Counter: 5\n",
      "Filtered   (009): ['I', 'ch', 'Ġmag', 'Ġ', 'Ã', 'Ħ', 'pf', 'el', '.']\n",
      "(decoder) Detokenized (003): ['Ich', 'Ġmag', 'ĠÃĦpfel.']\n",
      "(decoder) Counter: 9\n",
      "===================================================================\n",
      "Hidden states:  (13, 3, 768) (13, 3, 768)\n",
      "# Extracted words:  3 3\n",
      "(encoder) Sentence         : \"She drives a car.\"\n",
      "(encoder) Original    (004): ['She', 'drives', 'a', 'car.']\n",
      "(encoder) Tokenized   (009): ['<s>', 'S', 'he', 'Ġdr', 'ives', 'Ġa', 'Ġcar', '.', '</s>']\n",
      "(decoder) Sentence         : \"Sie fährt ein Auto.\"\n",
      "(decoder) Original    (004): ['Sie', 'fährt', 'ein', 'Auto.']\n",
      "(decoder) Tokenized   (012): ['<s>', 'S', 'ie', 'Ġf', 'Ã', '¤', 'h', 'rt', 'Ġein', 'ĠAuto', '.', '</s>']\n",
      "Filtered   (007): ['S', 'he', 'Ġdr', 'ives', 'Ġa', 'Ġcar', '.']\n",
      "(encoder) Detokenized (004): ['She', 'Ġdrives', 'Ġa', 'Ġcar.']\n",
      "(encoder) Counter: 7\n",
      "Filtered   (010): ['S', 'ie', 'Ġf', 'Ã', '¤', 'h', 'rt', 'Ġein', 'ĠAuto', '.']\n",
      "(decoder) Detokenized (004): ['Sie', 'ĠfÃ¤hrt', 'Ġein', 'ĠAuto.']\n",
      "(decoder) Counter: 10\n",
      "===================================================================\n",
      "Hidden states:  (13, 4, 768) (13, 4, 768)\n",
      "# Extracted words:  4 4\n",
      "(encoder) Sentence         : \"The sun is bright.\"\n",
      "(encoder) Original    (004): ['The', 'sun', 'is', 'bright.']\n",
      "(encoder) Tokenized   (007): ['<s>', 'The', 'Ġsun', 'Ġis', 'Ġbright', '.', '</s>']\n",
      "(decoder) Sentence         : \"Die Sonne ist hell.\"\n",
      "(decoder) Original    (004): ['Die', 'Sonne', 'ist', 'hell.']\n",
      "(decoder) Tokenized   (010): ['<s>', 'Die', 'ĠS', 'on', 'ne', 'Ġist', 'Ġh', 'ell', '.', '</s>']\n",
      "Filtered   (005): ['The', 'Ġsun', 'Ġis', 'Ġbright', '.']\n",
      "(encoder) Detokenized (004): ['The', 'Ġsun', 'Ġis', 'Ġbright.']\n",
      "(encoder) Counter: 5\n",
      "Filtered   (008): ['Die', 'ĠS', 'on', 'ne', 'Ġist', 'Ġh', 'ell', '.']\n",
      "(decoder) Detokenized (004): ['Die', 'ĠSonne', 'Ġist', 'Ġhell.']\n",
      "(decoder) Counter: 8\n",
      "===================================================================\n",
      "Hidden states:  (13, 4, 768) (13, 4, 768)\n",
      "# Extracted words:  4 4\n",
      "(encoder) Sentence         : \"He reads a book.\"\n",
      "(encoder) Original    (004): ['He', 'reads', 'a', 'book.']\n",
      "(encoder) Tokenized   (007): ['<s>', 'He', 'Ġreads', 'Ġa', 'Ġbook', '.', '</s>']\n",
      "(decoder) Sentence         : \"Er liest ein Buch.\"\n",
      "(decoder) Original    (004): ['Er', 'liest', 'ein', 'Buch.']\n",
      "(decoder) Tokenized   (009): ['<s>', 'Er', 'Ġli', 'est', 'Ġein', 'ĠB', 'uch', '.', '</s>']\n",
      "Filtered   (005): ['He', 'Ġreads', 'Ġa', 'Ġbook', '.']\n",
      "(encoder) Detokenized (004): ['He', 'Ġreads', 'Ġa', 'Ġbook.']\n",
      "(encoder) Counter: 5\n",
      "Filtered   (007): ['Er', 'Ġli', 'est', 'Ġein', 'ĠB', 'uch', '.']\n",
      "(decoder) Detokenized (004): ['Er', 'Ġliest', 'Ġein', 'ĠBuch.']\n",
      "(decoder) Counter: 7\n",
      "===================================================================\n",
      "Hidden states:  (13, 4, 768) (13, 4, 768)\n",
      "# Extracted words:  4 4\n",
      "(encoder) Sentence         : \"They live in a house.\"\n",
      "(encoder) Original    (005): ['They', 'live', 'in', 'a', 'house.']\n",
      "(encoder) Tokenized   (009): ['<s>', 'Th', 'ey', 'Ġlive', 'Ġin', 'Ġa', 'Ġhouse', '.', '</s>']\n",
      "(decoder) Sentence         : \"Sie wohnen in einem Haus.\"\n",
      "(decoder) Original    (005): ['Sie', 'wohnen', 'in', 'einem', 'Haus.']\n",
      "(decoder) Tokenized   (014): ['<s>', 'S', 'ie', 'Ġw', 'ohn', 'en', 'Ġin', 'Ġein', 'em', 'ĠH', 'a', 'us', '.', '</s>']\n",
      "Filtered   (007): ['Th', 'ey', 'Ġlive', 'Ġin', 'Ġa', 'Ġhouse', '.']\n",
      "(encoder) Detokenized (005): ['They', 'Ġlive', 'Ġin', 'Ġa', 'Ġhouse.']\n",
      "(encoder) Counter: 7\n",
      "Filtered   (012): ['S', 'ie', 'Ġw', 'ohn', 'en', 'Ġin', 'Ġein', 'em', 'ĠH', 'a', 'us', '.']\n",
      "(decoder) Detokenized (005): ['Sie', 'Ġwohnen', 'Ġin', 'Ġeinem', 'ĠHaus.']\n",
      "(decoder) Counter: 12\n",
      "===================================================================\n",
      "Hidden states:  (13, 5, 768) (13, 5, 768)\n",
      "# Extracted words:  5 5\n",
      "Loading activations...\n",
      "Loading json activations from encoder-activations-layer9.json...\n",
      "5 1.0\n",
      "Loading tokens...\n",
      "Preparing dataset...\n",
      "5it [00:00, 13156.54it/s]\n",
      "Writing datasets...\n",
      "Loading activations...\n",
      "Loading json activations from decoder-activations-layer9.json...\n",
      "5 1.0\n",
      "Loading tokens...\n",
      "Preparing dataset...\n",
      "5it [00:00, 13374.69it/s]\n",
      "Writing datasets...\n",
      "Min:  0  Max:  15  Del:  10000000\n",
      "Loading frequency\n",
      "Len of word dict: 18\n",
      "file sizes {0: 5}\n",
      "Loading  input.in.tok.sent_len_9-dataset.json\n",
      "Writing datasets...\n",
      "Limit Max types:  set()\n",
      "Skipped Min types:  set()\n",
      "Skipped frequent types:  set()\n",
      "Total word types before dropping:  18\n",
      "Total word tokens before dropping:  20\n",
      "Tokens skipped based on Max freq:  0\n",
      "Tokens skipped based on Min freq:  0\n",
      "Tokens skipped based on Del freq:  0\n",
      "Types skipped based on Max freq:  0\n",
      "Types skipped based on Min freq:  0\n",
      "Types skipped based on Del freq:  0\n",
      "Remaining Tokens:  20\n",
      "Remaining Types:  18\n",
      "Min:  0  Max:  15  Del:  10000000\n",
      "Loading frequency\n",
      "Len of word dict: 18\n",
      "file sizes {0: 5}\n",
      "Loading  label.out.tok.sent_len_9-dataset.json\n",
      "Writing datasets...\n",
      "Limit Max types:  set()\n",
      "Skipped Min types:  set()\n",
      "Skipped frequent types:  set()\n",
      "Total word types before dropping:  18\n",
      "Total word tokens before dropping:  20\n",
      "Tokens skipped based on Max freq:  0\n",
      "Tokens skipped based on Min freq:  0\n",
      "Tokens skipped based on Del freq:  0\n",
      "Types skipped based on Max freq:  0\n",
      "Types skipped based on Min freq:  0\n",
      "Types skipped based on Del freq:  0\n",
      "Remaining Tokens:  20\n",
      "Remaining Types:  18\n",
      "Reading input.in.tok.sent_len_9_min_0_max_15_del_10000000-dataset.json\n",
      "vocab file: encoder-processed-vocab-9.npy\n",
      "point file: encoder-processed-point-9.npy\n",
      "Written vocab file and point file\n",
      "Reading label.out.tok.sent_len_9_min_0_max_15_del_10000000-dataset.json\n",
      "vocab file: decoder-processed-vocab-9.npy\n",
      "point file: decoder-processed-point-9.npy\n",
      "Written vocab file and point file\n"
     ]
    }
   ],
   "source": [
    "!./activation_extraction_without_filtering.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "65fb3dcb-bde6-46fb-9565-d622fd635958",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USAGE: create_kmeans_clustering.py -p <POINT_FILE> -v <VOCAB_FILE> -k <CLUSTERS> -o <OUTPUT_FOLDER>\n",
      "Initialization complete\n",
      "Iteration 0, inertia 40975.94307993702\n",
      "Iteration 1, inertia 25837.016537789867\n",
      "Converged at iteration 1: strict convergence.\n",
      "Initialization complete\n",
      "Iteration 0, inertia 38981.26510065491\n",
      "Iteration 1, inertia 24143.224373957477\n",
      "Converged at iteration 1: strict convergence.\n",
      "Initialization complete\n",
      "Iteration 0, inertia 45696.19739106346\n",
      "Iteration 1, inertia 24290.395948137015\n",
      "Converged at iteration 1: strict convergence.\n",
      "Initialization complete\n",
      "Iteration 0, inertia 41495.874363051116\n",
      "Iteration 1, inertia 23709.24160222024\n",
      "Converged at iteration 1: strict convergence.\n",
      "Initialization complete\n",
      "Iteration 0, inertia 37964.97862997214\n",
      "Iteration 1, inertia 23592.88948380079\n",
      "Converged at iteration 1: strict convergence.\n",
      "Initialization complete\n",
      "Iteration 0, inertia 38434.625113773705\n",
      "Iteration 1, inertia 23347.071066886314\n",
      "Iteration 2, inertia 22695.88654895659\n",
      "Converged at iteration 2: strict convergence.\n",
      "Initialization complete\n",
      "Iteration 0, inertia 37686.26830506383\n",
      "Iteration 1, inertia 23168.433291432942\n",
      "Iteration 2, inertia 22549.022467286813\n",
      "Converged at iteration 2: strict convergence.\n",
      "Initialization complete\n",
      "Iteration 0, inertia 41014.38670620435\n",
      "Iteration 1, inertia 22416.655640516645\n",
      "Converged at iteration 1: strict convergence.\n",
      "Initialization complete\n",
      "Iteration 0, inertia 38842.886389029816\n",
      "Iteration 1, inertia 24013.52427795189\n",
      "Converged at iteration 1: strict convergence.\n",
      "Initialization complete\n",
      "Iteration 0, inertia 40762.957995216886\n",
      "Iteration 1, inertia 23681.860498329777\n",
      "Converged at iteration 1: strict convergence.\n",
      "Filename: code/create_kmeans_clustering.py\n",
      "\n",
      "Line #    Mem usage    Increment  Occurrences   Line Contents\n",
      "=============================================================\n",
      "    10    106.8 MiB    106.8 MiB           1   @profile\n",
      "    11                                         def kmeans_cluster(P, V, K, output_path, ref=''):\n",
      "    12                                             \"\"\"\n",
      "    13                                             Uses the point.npy P, vocab.npy V files of a layer (generated using https://github.com/hsajjad/ConceptX/ library) to produce a clustering of <K> clusters at <output_path> named clusters-kmeans-{K}.txt\n",
      "    14                                             \"\"\"\n",
      "    15    106.8 MiB      0.0 MiB           1       kmeans = KMeans(n_clusters=K, verbose=3, random_state=212)\n",
      "    16    108.5 MiB      1.8 MiB           1       output = kmeans.fit(P)\n",
      "    17                                             \n",
      "    18    108.5 MiB      0.0 MiB           1       out_file =  f\"{output_path}/clusters-kmeans-{K}{ref}.txt\"\n",
      "    19                                         \n",
      "    20    108.5 MiB      0.0 MiB           6       clusters = {i:[] for i in range(K)}\n",
      "    21                                             \n",
      "    22    108.5 MiB      0.0 MiB          20       for v, l in zip(V, output.labels_):\n",
      "    23    108.5 MiB      0.0 MiB          19          clusters[l].append(f'{v}|||{l}')\n",
      "    24                                         \n",
      "    25                                         \n",
      "    26    108.5 MiB      0.0 MiB           1       out = \"\"\n",
      "    27    108.5 MiB      0.0 MiB           4       for k,v in clusters.items():\n",
      "    28    108.5 MiB      0.0 MiB           3           out += '\\n'.join(v) + '\\n'\n",
      "    29                                         \n",
      "    30                                             \n",
      "    31    108.5 MiB      0.0 MiB           1       with open(out_file, 'w') as f2:\n",
      "    32    108.5 MiB      0.0 MiB           1           f2.write(out)\n",
      "    33                                         \n",
      "    34    108.5 MiB      0.0 MiB           1       return out\n",
      "\n",
      "\n",
      "Runtime: 0.15763568878173828\n"
     ]
    }
   ],
   "source": [
    "!./clustering.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70ef036d-c2a2-4f2c-b21c-0e73b0bf8ea8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b57f7cf-0058-40a3-9f10-afd717dba1b5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "neurox_pip",
   "language": "python",
   "name": "neurox_pip"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
