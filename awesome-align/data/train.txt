void add_100 ( int numElements , int * data ) { for ( int idx = 0 ; idx < numElements ; idx ++ ) { data [ idx ] += 100 ; } } ||| __global__ void add_100 ( int numElements , int * data ) { if ( blockIdx . x < numElements ) { data [ blockIdx . x ] += 100 ; } }
void get_ev ( double * old_arr , double * new_arr , int size ) { int tid ; for ( tid = 0 ; tid < size ; tid ++ ) new_arr [ tid ] = old_arr [ tid ] ; } ||| __global__ void get_ev ( double * old_arr , double * new_arr ) { int tid = threadIdx . x + blockIdx . x * blockDim . x ; new_arr [ tid ] = old_arr [ tid ] ; }
void square ( int * array , int arrayCount ) { for ( int idx = 0 ; idx < arrayCount ; idx ++ ) { array [ idx ] *= array [ idx ] ; } } ||| __global__ void square ( int * array , int arrayCount ) { int idx = threadIdx . x + blockIdx . x * blockDim . x ; if ( idx < arrayCount ) { array [ idx ] *= array [ idx ] ; } }
void add ( int n , float * x , float * y ) { for ( int i = 0 ; i < n ; i ++ ) y [ i ] = x [ i ] + y [ i ] ; } ||| __global__ void add ( int n , float * x , float * y ) { int i = threadIdx . x ; if ( i < n ) y [ i ] = x [ i ] + y [ i ] ; }
void scale_host ( float * array , float scale , int N ) { for ( int idx = 0 ; idx < N ; idx ++ ) { array [ idx ] *= scale ; } return ; } ||| __global__ void scale_dev ( float * array , float scale , int N ) { int idx = blockIdx . x * blockDim . x + threadIdx . x ; if ( idx < N ) { array [ idx ] *= scale ; } return ; }
void allAddInplace_cpu ( double * arr , double alpha , int n ) { for ( int i = 0 ; i < n ; i ++ ) { arr [ i ] += alpha ; } } ||| __global__ void allAddInplaceKernel ( double * arr , double alpha , int n ) { int i = blockIdx . x * blockDim . x + threadIdx . x ; if ( i < n ) { arr [ i ] += alpha ; } }
void memsetCpuInt ( int * data , int val , int N ) { for ( int index = 0 ; index < N ; index ++ ) { data [ index ] = val ; } } ||| __global__ void memsetCudaInt ( int * data , int val , int N ) { unsigned int index = blockDim . x * blockIdx . x + threadIdx . x ; if ( index >= N ) { return ; } data [ index ] = val ; }
void initialArray0_cpu ( int tasks , int * f3 ) { for ( int i = 0 ; i < tasks ; i ++ ) { f3 [ i ] = 0 ; } } ||| __global__ void initialArray0 ( int tasks , int * f3 ) { for ( int i = blockIdx . x * blockDim . x + threadIdx . x ; i < tasks ; i += blockDim . x * gridDim . x ) { f3 [ i ] = 0 ; } }
void add_vector_cpu ( float * a , float * b , float * c , int size ) { for ( int i = 0 ; i < size ; ++ i ) c [ i ] = a [ i ] + b [ i ] ; } ||| __global__ void VectorAdd ( float * arrayA , float * arrayB , float * output ) { int idx = threadIdx . x ; output [ idx ] = arrayA [ idx ] + arrayB [ idx ] ; }
void test_cpu ( float * input , const int dims ) { for ( int tid = 0 ; tid < dims ; tid ++ ) { if ( tid == 0 ) { input [ tid ] = 0 ; } } } ||| __global__ void test ( float * input , const int dims ) { int tid = blockIdx . x * blockDim . x + threadIdx . x ; if ( tid >= dims ) { return ; } if ( tid == 0 ) { input [ tid ] = 0 ; } }
void set_sorting_offset ( const int nrows , const int ncols , int * offsets ) { int tid ; for ( tid = 0 ; tid <= ncols ; tid ++ ) offsets [ tid ] = tid * nrows ; return ; } ||| __global__ void set_sorting_offset ( const int nrows , const int ncols , int * offsets ) { int tid = threadIdx . x + blockIdx . x * blockDim . x ; if ( tid <= ncols ) offsets [ tid ] = tid * nrows ; return ; }
void dot_cpu ( float * c , float * a , float * b , int size ) { int t_id ; for ( t_id = 0 ; t_id < size ; t_id ++ ) c [ t_id ] = a [ t_id ] * b [ t_id ] ; } ||| __global__ void dotKernel ( float * c , float * a , float * b ) { int t_id = blockIdx . x * blockDim . x + threadIdx . x ; c [ t_id ] = a [ t_id ] * b [ t_id ] ; }
void matDiagAddInplace_cpu ( double * mat , double alpha , int dim ) { for ( int i = 0 ; i < dim ; i ++ ) { mat [ i * dim + i ] += alpha ; } } ||| __global__ void matDiagAddInplaceKernel ( double * mat , double alpha , int dim ) { int i = blockIdx . x * blockDim . x + threadIdx . x ; if ( i < dim ) { mat [ i * dim + i ] += alpha ; } }
void cpuAddCorrAndCorrection ( float * L , float * r , int N ) { for ( int u = 0 ; u < N ; u ++ ) { L [ u ] -= r [ u ] ; } } ||| __global__ void cudaAddCorrAndCorrection ( float * L , float * r , int N ) { int u = ( blockIdx . x * blockDim . x ) + threadIdx . x ; if ( u >= N ) return ; L [ u ] -= r [ u ] ; }
void fill_cpu ( int N , float ALPHA , float * X , int INCX ) { int i ; for ( i = 0 ; i < N ; ++ i ) X [ i * INCX ] = ALPHA ; } ||| __global__ void fill_kernel ( int N , float ALPHA , float * X , int INCX ) { int i = ( blockIdx . x + blockIdx . y * gridDim . x ) * blockDim . x + threadIdx . x ; if ( i < N ) X [ i * INCX ] = ALPHA ; }
void scal_cpu ( int N , float ALPHA , float * X , int INCX ) { int i ; for ( i = 0 ; i < N ; ++ i ) X [ i * INCX ] *= ALPHA ; } ||| __global__ void scal_kernel ( int N , float ALPHA , float * X , int INCX ) { int i = ( blockIdx . x + blockIdx . y * gridDim . x ) * blockDim . x + threadIdx . x ; if ( i < N ) X [ i * INCX ] *= ALPHA ; }
void PSIfill_cpu ( float * array , int conv_length , int n ) { for ( int i = 0 ; i < n ; i ++ ) { array [ i ] = array [ i % conv_length ] ; } } ||| __global__ void PSIfill ( float * array , int conv_length , int maxThreads ) { int i = blockIdx . x * blockDim . x + threadIdx . x ; if ( i >= maxThreads ) return ; array [ i ] = array [ i % conv_length ] ; }
void host_add ( float * c , float * a , float * b , int n ) { for ( int k = 0 ; k < n ; k ++ ) { c [ k ] = a [ k ] + b [ k ] ; } } ||| __global__ void gpu_add ( float * c , float * a , float * b , int n ) { int j = blockIdx . x * blockDim . x + threadIdx . x ; c [ j ] = a [ j ] + b [ j ] ; }
void mul_Scalar_matrix ( float * a , float value , float * c , int N ) { for ( int idx = 0 ; idx < N ; idx ++ ) { c [ idx ] = a [ idx ] * value ; } } ||| __global__ void mul_Scalar_matrix ( float * a , float value , float * c , int N ) { int idx = blockIdx . x * blockDim . x + threadIdx . x ; if ( idx < N ) c [ idx ] = a [ idx ] * value ; }
void initWith_cpu ( float num , float * a , int N ) { for ( int i = 0 ; i < N ; i ++ ) { a [ i ] = num ; } } ||| __global__ void initWith ( float num , float * a , int N ) { int index = threadIdx . x + blockIdx . x * blockDim . x ; int stride = blockDim . x * gridDim . x ; for ( int i = index ; i < N ; i += stride ) { a [ i ] = num ; } }
void zeroIndices_cpu ( long * vec_out , const long N ) { for ( int idx = 0 ; idx < N ; idx ++ ) { vec_out [ idx ] = vec_out [ idx ] - vec_out [ 0 ] ; } } ||| __global__ void zeroIndices ( long * vec_out , const long N ) { int idx = threadIdx . x + blockDim . x * blockIdx . x ; if ( idx < N ) { vec_out [ idx ] = vec_out [ idx ] - vec_out [ 0 ] ; } }
void saxpy_serial ( const int dim , float a , float * x , float * y ) { for ( int i = 0 ; i < dim ; i ++ ) y [ i ] += a * x [ i ] ; } ||| __global__ void saxpy_gpu ( const int dim , float a , float * x , float * y ) { int i = blockIdx . x * blockDim . x + threadIdx . x ; if ( i < dim ) y [ i ] = a * x [ i ] + y [ i ] ; }
void getCanBusData ( int * canData , int size ) { int idx ; for ( idx = 0 ; idx < size ; idx ++ ) { canData [ idx ] += 1 ; } } ||| __global__ void getCanBusData ( int * canData , int size , int nthreads , int nblocks ) { int i ; int idx = blockIdx . x * blockDim . x + threadIdx . x ; for ( i = idx ; i < size ; i += nthreads * nblocks ) { canData [ idx ] += 1 ; } }
void sum_array_cpu ( float * a , float * b , float * c , const int size ) { for ( int i = 0 ; i < size ; ++ i ) { c [ i ] = a [ i ] + b [ i ] ; } } ||| __global__ void sum_array_1Dgrid_1Dblock ( float * a , float * b , float * c , int nx ) { int gid = blockDim . x * blockIdx . x + threadIdx . x ; c [ gid ] = a [ gid ] + b [ gid ] ; }
void matColMeanDiv_cpu ( double * buf , int m , int n , double * tmp ) { for ( int i = 0 ; i < n ; i ++ ) { buf [ i ] = tmp [ i ] / m ; } } ||| __global__ void matColMeanDiv ( double * buf , int m , int n , double * tmp ) { int i = blockIdx . x * blockDim . x + threadIdx . x ; if ( i < n ) { buf [ i ] = tmp [ i ] / m ; } }
void dmul_Scalar_matrix ( double * a , double value , double * c , int N ) { for ( int idx = 0 ; idx < N ; idx ++ ) { c [ idx ] = a [ idx ] * value ; } } ||| __global__ void dmul_Scalar_matrix ( double * a , double value , double * c , int N ) { int idx = blockIdx . x * blockDim . x + threadIdx . x ; if ( idx < N ) c [ idx ] = a [ idx ] * value ; }
void countRangesGlobal ( int size , int * A , int * B ) { for ( int i = 0 ; i < size ; i ++ ) { int x = A [ i ] / 100 ; B [ x ] += 1 ; } } ||| __global__ void countRangesGlobal ( int size , int * A , int * B ) { int i = blockIdx . x * blockDim . x + threadIdx . x ; if ( i >= size ) return ; int x = A [ i ] / 100 ; B [ x ] += 1 ; }
void dsubtract_matrix ( double * a , double * b , double * c , int N ) { for ( int idx = 0 ; idx < N ; idx ++ ) { c [ idx ] = a [ idx ] - b [ idx ] ; } } ||| __global__ void dsubtract_matrix ( double * a , double * b , double * c , int N ) { int idx = blockIdx . x * blockDim . x + threadIdx . x ; if ( idx < N ) c [ idx ] = a [ idx ] - b [ idx ] ; }
void add_arrays ( int n , float * x , float * y , float * z ) { for ( int i = 0 ; i < n ; i ++ ) { z [ i ] = x [ i ] + y [ i ] ; } } ||| __global__ void add_arrays ( int n , float * x , float * y , float * z ) { int i = blockDim . x * blockIdx . x + threadIdx . x ; if ( i < n ) { z [ i ] = x [ i ] + y [ i ] ; } }
void sum_arrays_cpu ( int * a , int * b , int * c , int size ) { for ( int i = 0 ; i < size ; i ++ ) { c [ i ] = a [ i ] + b [ i ] ; } } ||| __global__ void sum_arrays_gpu ( int * a , int * b , int * c , int size ) { int index = blockDim . x * blockIdx . x + threadIdx . x ; if ( index < size ) c [ index ] = a [ index ] + b [ index ] ; }
void iKernel_cpu ( float * A , float * B , float * C , const int N ) { for ( int i = 0 ; i < N ; i ++ ) { C [ i ] = A [ i ] + B [ i ] ; } } ||| __global__ void iKernel ( float * A , float * B , float * C , const int N ) { int i = blockIdx . x * blockDim . x + threadIdx . x ; if ( i < N ) C [ i ] = A [ i ] + B [ i ] ; }
void multiplyIntValues ( int * destination , int * vector , int value , unsigned int end ) { for ( unsigned int i = 0 ; i < end ; i ++ ) { destination [ i ] = vector [ i ] * value ; } } ||| __global__ void intMultiply ( int * result , const int * val1 , const int val2 , const unsigned int size ) { int i = threadIdx . x + blockIdx . x * blockDim . x ; if ( i < size ) { result [ blockIdx . x ] = val1 [ blockIdx . x ] * val2 ; } }
void doubleArrayScalarDivide_cpu ( double * d_in , int * d_out , int length , double scalar ) { for ( int idx = 0 ; idx < length ; idx ++ ) { d_out [ idx ] = d_in [ idx ] / scalar ; } } ||| __global__ void doubleArrayScalarDivideKernel ( double * d_in , int * d_out , int length , double scalar ) { int tid = ( blockIdx . x * blockDim . x ) + threadIdx . x ; if ( tid < length ) { d_out [ tid ] = ( int ) ( d_in [ tid ] / scalar ) ; } }
void add ( const int x , const int y , const int WIDTH , int * c , const int * a , const int * b ) { int i = y * ( WIDTH ) + x ; c [ i ] = a [ i ] + b [ i ] ; } ||| __global__ void addKernel ( int * c , const int * a , const int * b ) { int x = threadIdx . x ; int y = threadIdx . y ; int i = y * ( blockDim . x ) + x ; c [ i ] = a [ i ] + b [ i ] ; }
void activate_array_leaky_cpu ( float * x , int n ) { for ( int index = 0 ; index < n ; index ++ ) { float val = x [ index ] ; x [ index ] = ( val > 0 ) ? val : val / 10 ; } } ||| __global__ void activate_array_leaky_kernel ( float * x , int n ) { int index = blockIdx . x * blockDim . x + threadIdx . x ; if ( index < n ) { float val = x [ index ] ; x [ index ] = ( val > 0 ) ? val : val / 10 ; } }
void logistic_cpu ( unsigned int n , float a , float * x , float * z ) { for ( int myId = 0 ; myId < n ; myId ++ ) { z [ myId ] = a * x [ myId ] * ( 1 - x [ myId ] ) ; } } ||| __global__ void logistic ( unsigned int n , float a , float * x , float * z ) { unsigned int myId = blockDim . x * blockIdx . x + threadIdx . x ; if ( myId < n ) z [ myId ] = a * x [ myId ] * ( 1 - x [ myId ] ) ; }
void add_kernel ( float * inputleft , float * inputright , float * output , int count ) { int idx ; for ( idx = 0 ; idx < count ; idx ++ ) output [ idx ] = inputleft [ idx ] + inputright [ idx ] ; } ||| __global__ void add_kernel ( float * inputleft , float * inputright , float * output , int count ) { int idx = threadIdx . x + blockDim . x * blockIdx . x ; if ( idx >= count ) return ; output [ idx ] = inputleft [ idx ] + inputright [ idx ] ; }
void mul_cpu ( int N , float * X , int INCX , float * Y , int INCY ) { int i ; for ( i = 0 ; i < N ; ++ i ) Y [ i * INCY ] *= X [ i * INCX ] ; } ||| __global__ void mul_kernel ( int N , float * X , int INCX , float * Y , int INCY ) { int i = ( blockIdx . x + blockIdx . y * gridDim . x ) * blockDim . x + threadIdx . x ; if ( i < N ) Y [ i * INCY ] *= X [ i * INCX ] ; }
void pathPlan ( int * devSpeed , int * devSteer , int size ) { int tid ; for ( tid = 0 ; tid < size ; tid ++ ) { devSpeed [ tid ] += 1 ; devSteer [ tid ] += 1 ; } } ||| __global__ void pathPlan ( int * devSpeed , int * devSteer , int size ) { int tid = threadIdx . x + blockIdx . x * blockDim . x ; while ( tid < size ) { devSpeed [ tid ] += 1 ; devSteer [ tid ] += 1 ; tid += blockDim . x * gridDim . x ; } }
void mult_add_into_cpu ( int N , float * X , float * Y , float * Z ) { int i ; for ( i = 0 ; i < N ; ++ i ) Z [ i ] += X [ i ] * Y [ i ] ; } ||| __global__ void mult_add_into_kernel ( int n , float * a , float * b , float * c ) { int i = ( blockIdx . x + blockIdx . y * gridDim . x ) * blockDim . x + threadIdx . x ; if ( i < n ) { c [ i ] += a [ i ] * b [ i ] ; } }
void InitReduction ( int * flags , int voxelCount , int * reduction , int reductionSize ) { int tid ; for ( tid = 0 ; tid < reductionSize ; tid ++ ) reduction [ tid ] = ( tid < voxelCount ) ? flags [ tid ] : 0 ; } ||| __global__ void InitReduction ( bool * flags , int voxelCount , int * reduction , int reductionSize ) { int tid = threadIdx . x + blockIdx . x * blockDim . x ; if ( tid >= reductionSize ) { return ; } reduction [ tid ] = ( tid < voxelCount ) ? flags [ tid ] : 0 ; }
void Function_update_sgd_cpu ( float lr , float * parameter , float * gradient , int size ) { for ( int i = 0 ; i < size ; i ++ ) parameter [ i ] -= lr * gradient [ i ] ; } ||| __global__ void Kernel_Function_update_sgd ( float lr , float * dev_parameter , float * dev_gradient , int size ) { int tid = blockDim . x * blockIdx . x + threadIdx . x ; int N = size ; while ( tid < N ) { dev_parameter [ tid ] -= lr * dev_gradient [ tid ] ; tid += gridDim . x * blockDim . x ; } }
void operacionCPU ( float * u , float * lu , float u_m , float u_d , int n ) { int idx = 0 ; while ( idx < n ) { lu [ idx ] = ( u [ idx ] - u_m ) / u_d ; idx += 1 ; } } ||| __global__ void operacionKernelGPU ( float * u , float * lu , float u_m , float u_d , int n ) { int idx = threadIdx . x + blockDim . x * blockIdx . x ; if ( idx < n ) lu [ idx ] = ( u [ idx ] - u_m ) / u_d ; }
void host_add ( float * c , float * a , float * b , int n ) { for ( int k = 0 ; k < n ; k ++ ) { c [ k ] = a [ k ] + b [ k ] ; } } ||| __global__ void gpu_add ( float * c , float * a , float * b , int n ) { int j = blockIdx . x * blockDim . x + threadIdx . x ; int m = gridDim . x * blockDim . x ; for ( int k = j ; k < n ; k += m ) { c [ k ] = a [ k ] + b [ k ] ; } }
void squareSerial ( float * d_in , float * d_out , int N ) { for ( unsigned int i = 0 ; i < N ; ++ i ) { d_out [ i ] = pow ( d_in [ i ] / ( d_in [ i ] - 2.3 ) , 3 ) ; } } ||| __global__ void squareKernel ( float * d_in , float * d_out , int N ) { const unsigned int lid = threadIdx . x ; const unsigned int gid = blockIdx . x * blockDim . x + lid ; if ( gid < N ) { d_out [ gid ] = pow ( d_in [ gid ] / ( d_in [ gid ] - 2.3 ) , 3 ) ; } }
void doubleArrayVectorAdd_cpu ( double * d_in_a , double * d_in_b , double * d_out , int length ) { for ( int idx = 0 ; idx < length ; idx ++ ) { d_out [ idx ] = d_in_a [ idx ] + d_in_b [ idx ] ; } } ||| __global__ void doubleArrayVectorAddKernel ( double * d_in_a , double * d_in_b , double * d_out , int length ) { int tid = ( blockIdx . x * blockDim . x ) + threadIdx . x ; if ( tid < length ) { d_out [ tid ] = d_in_a [ tid ] + d_in_b [ tid ] ; } }
void fill_matrix ( double * const A , const int rows , const int cols ) { int row , col ; for ( row = 0 ; row < rows ; row ++ ) { for ( col = 0 ; col < cols ; col ++ ) { A [ row * cols + col ] = row ; } } } ||| __global__ void fill_matrix ( double * const A , const int rows , const int cols ) { const int row = blockIdx . y * blockDim . y + threadIdx . y , col = blockIdx . x * blockDim . x + threadIdx . x ; if ( row < rows && col < cols ) { A [ row * cols + col ] = row ; } }
void evenoddincrement_cpu ( float * g_data , int even_inc , int odd_inc , int size ) { int tx ; for ( tx = 0 ; tx < size ; tx ++ ) { if ( ( tx % 2 ) == 0 ) { g_data [ tx ] += even_inc ; } else { g_data [ tx ] += odd_inc ; } } } ||| __global__ void evenoddincrement ( float * g_data , int even_inc , int odd_inc ) { int tx = threadIdx . x + blockIdx . x * blockDim . x ; if ( ( tx % 2 ) == 0 ) { g_data [ tx ] += even_inc ; } else { g_data [ tx ] += odd_inc ; } }
void copy_cpu ( int N , float * X , int INCX , float * Y , int INCY ) { int i ; for ( i = 0 ; i < N ; ++ i ) Y [ i * INCY ] = X [ i * INCX ] ; } ||| __global__ void copy_kernel ( int N , float * X , int OFFX , int INCX , float * Y , int OFFY , int INCY ) { int i = ( blockIdx . x + blockIdx . y * gridDim . x ) * blockDim . x + threadIdx . x ; if ( i < N ) Y [ i * INCY + OFFY ] = X [ i * INCX + OFFX ] ; }
void clearLabel ( float * prA , float * prB , unsigned int num_nodes , float base ) { unsigned int id ; for ( id = 0 ; id < num_nodes ; id ++ ) { prA [ id ] = base + prA [ id ] * 0.85 ; prB [ id ] = 0 ; } } ||| __global__ void clearLabel ( float * prA , float * prB , unsigned int num_nodes , float base ) { unsigned int id = blockDim . x * blockIdx . x + threadIdx . x ; if ( id < num_nodes ) { prA [ id ] = base + prA [ id ] * 0.85 ; prB [ id ] = 0 ; } }
void delay_kernel_cpu ( int * N_mobil , int * Tau , int dia ) { int N = N_mobil [ 0 ] ; for ( int id = 0 ; id < N ; id ++ ) { if ( Tau [ id ] > 0 ) Tau [ id ] = Tau [ id ] - 1 ; } } ||| __global__ void delay_kernel ( int * N_mobil , int * Tau , int dia ) { int N = N_mobil [ 0 ] ; int id = blockIdx . x * blockDim . x + threadIdx . x ; if ( id < N ) { if ( Tau [ id ] > 0 ) Tau [ id ] = Tau [ id ] - 1 ; } }
void resetHeap_cpu ( int * heap , int * heapPtr , int numBlock ) { for ( int index = 0 ; index < numBlock ; index ++ ) { if ( index == 0 ) heapPtr [ 0 ] = numBlock - 1 ; heap [ index ] = numBlock - index - 1 ; } } ||| __global__ void resetHeapKernel ( int * heap , int * heapPtr , int numBlock ) { int index = threadIdx . x + blockDim . x * blockIdx . x ; if ( index >= numBlock ) return ; if ( index == 0 ) heapPtr [ 0 ] = numBlock - 1 ; heap [ index ] = numBlock - index - 1 ; }
void pow_cpu ( int N , float ALPHA , float * X , int INCX , float * Y , int INCY ) { int i ; for ( i = 0 ; i < N ; ++ i ) Y [ i * INCY ] = pow ( X [ i * INCX ] , ALPHA ) ; } ||| __global__ void pow_kernel ( int N , float ALPHA , float * X , int INCX , float * Y , int INCY ) { int i = ( blockIdx . x + blockIdx . y * gridDim . x ) * blockDim . x + threadIdx . x ; if ( i < N ) Y [ i * INCY ] = pow ( X [ i * INCX ] , ALPHA ) ; }
void kComputeActs ( const float * d_nets , float * d_acts , int size ) { int un_idx = 0 ; for ( un_idx = 0 ; un_idx < size ; un_idx ++ ) { float tact = 1.0f / ( 1.0f + expf ( - d_acts [ un_idx ] ) ) ; d_acts [ un_idx ] = tact ; } } ||| __global__ void kComputeActs ( const float * d_nets , float * d_acts ) { int un_idx = blockIdx . x * blockDim . x + threadIdx . x ; float tact = 1.0f / ( 1.0f + expf ( - d_acts [ un_idx ] ) ) ; __syncthreads ( ) ; d_acts [ un_idx ] = tact ; }
void transpositionCPU ( int * vector , int * transposed , int size ) { for ( int i = 0 ; i < size ; i ++ ) for ( int j = 0 ; j < size ; j ++ ) transposed [ i + j * size ] = vector [ j + i * size ] ; } ||| __global__ void transposeNaive ( int * vector , int * transposed , int size ) { int column = threadIdx . x + blockDim . x * blockIdx . x ; int row = threadIdx . y + blockDim . x * blockIdx . y ; if ( row < size && column < size ) transposed [ row + column * size ] = vector [ column + row * size ] ; }
void compute_array_square ( float * array , float * outArray , int size ) { for ( int i = 0 ; i < size ; i ++ ) outArray [ i ] = array [ i ] * array [ i ] ; } ||| __global__ void compute_array_square ( float * array , float * outArray , int size ) { int thread_index = threadIdx . x + blockIdx . x * blockDim . x ; int num_threads = blockDim . x * gridDim . x ; for ( int i = 0 ; i < size ; i += num_threads ) { int index = i + thread_index ; if ( index < size ) { outArray [ index ] = array [ index ] * array [ index ] ; } } }
void testInt1_cpu ( const int * input , int dims ) { for ( int tid = 0 ; tid < dims ; tid ++ ) { int sum ; for ( int i = 0 ; i < 3000 * 4 ; i ++ ) { if ( input [ i ] == 0 ) { sum ++ ; } } } } ||| __global__ void testInt1 ( const int * input , int dims ) { int tid = blockIdx . x * blockDim . x + threadIdx . x ; if ( tid >= dims ) { return ; } int sum ; for ( int i = 0 ; i < 3000 * 4 ; i ++ ) { if ( input [ i ] == 0 ) { sum ++ ; } } }
void incKernel ( int * g_out , int * g_in , int N , int inner_reps ) { for ( int idx = 0 ; idx < N ; idx ++ ) { for ( int i = 0 ; i < inner_reps ; ++ i ) { g_out [ idx ] = g_in [ idx ] + 1 ; } } } ||| __global__ void incKernel ( int * g_out , int * g_in , int N , int inner_reps ) { int idx = blockIdx . x * blockDim . x + threadIdx . x ; if ( idx < N ) { for ( int i = 0 ; i < inner_reps ; ++ i ) { g_out [ idx ] = g_in [ idx ] + 1 ; } } }
void forward_dropout_layer ( int batch , int inputs , float * input , float probability , float * rand , float scale ) { int i ; for ( i = 0 ; i < batch * inputs ; ++ i ) { if ( rand [ i ] < probability ) input [ i ] = 0 ; else input [ i ] *= scale ; } } ||| __global__ void forward_dropout_layer ( float * input , int size , float * rand , float prob , float scale ) { int id = ( blockIdx . x + blockIdx . y * gridDim . x ) * blockDim . x + threadIdx . x ; if ( id < size ) input [ id ] = ( rand [ id ] < prob ) ? 0 : input [ id ] * scale ; }
void boundaryCorrectIndexes_cpu ( int * d_in , int * d_out , int length , int N ) { for ( int idx = 0 ; idx < length ; idx ++ ) { if ( d_in [ idx ] > N ) { d_out [ idx ] = N ; } else { d_out [ idx ] = d_in [ idx ] ; } } } ||| __global__ void boundaryCorrectIndexesKernel ( int * d_in , int * d_out , int length , int N ) { int tid = ( blockIdx . x * blockDim . x ) + threadIdx . x ; if ( tid < length ) { if ( d_in [ tid ] > N ) { d_out [ tid ] = N ; } else { d_out [ tid ] = d_in [ tid ] ; } } }
void upsweep_scan ( int twod , int N , int * output ) { int twod1 = twod * 2 ; int idx ; for ( idx = 0 ; idx + twod1 - 1 < N ; idx += twod1 ) output [ idx + twod1 - 1 ] += output [ idx + twod - 1 ] ; } ||| __global__ void upsweep_scan ( int twod , int N , int * output ) { int twod1 = twod * 2 ; int idx = ( blockIdx . x * blockDim . x + threadIdx . x ) * twod1 ; if ( idx + twod1 - 1 < N ) output [ idx + twod1 - 1 ] += output [ idx + twod - 1 ] ; }
void Blend_CPU ( unsigned char * aImg1 , unsigned char * aImg2 , unsigned char * aRS , int width , int height ) { for ( int i = 0 ; i < width * height ; ++ i ) aRS [ i ] = ( unsigned char ) ( 0.5 * aImg1 [ i ] + 0.5 * aImg2 [ i ] ) ; } ||| __global__ void Blending_Kernel ( unsigned char * aR1 , unsigned char * aR2 , unsigned char * aRS , int size ) { int index = blockIdx . x * blockDim . x + threadIdx . x ; if ( index < size ) aRS [ index ] = 0.5 * aR1 [ index ] + 0.5 * aR2 [ index ] ; }
void matVecRowSubInplace_cpu ( double * mat , const double * vec , int m , int n ) { for ( int index = 0 ; index < m * n ; index ++ ) { int i = index / n ; int j = index % n ; mat [ i * n + j ] -= vec [ j ] ; } } ||| __global__ void matVecRowSubInplaceKernel ( double * mat , const double * vec , int m , int n ) { int index = blockIdx . x * blockDim . x + threadIdx . x ; if ( index < m * n ) { int i = index / n ; int j = index % n ; mat [ i * n + j ] -= vec [ j ] ; } }
void matVecColAddInplace_cpu ( double * mat , const double * vec , int m , int n ) { for ( int index = 0 ; index < m * n ; index ++ ) { int i = index / n ; int j = index % n ; mat [ i * n + j ] += vec [ i ] ; } } ||| __global__ void matVecColAddInplaceKernel ( double * mat , const double * vec , int m , int n ) { int index = blockIdx . x * blockDim . x + threadIdx . x ; if ( index < m * n ) { int i = index / n ; int j = index % n ; mat [ i * n + j ] += vec [ i ] ; } }
void MMDOuterProdComputeWithSum ( float * x_average , int size_x , float * x_outer_prod ) { for ( int i = 0 ; i < size_x ; i ++ ) { x_outer_prod [ i ] = x_average [ i ] * x_average [ i ] ; } } ||| __global__ void MMDOuterProdComputeWithSum ( float * x_average , int size_x , float * x_outer_prod ) { int block_id = blockIdx . x ; int thread_id = threadIdx . x ; for ( int i = block_id * blockDim . x + thread_id ; i < size_x ; i += gridDim . x * blockDim . x ) { x_outer_prod [ i ] = x_average [ i ] * x_average [ i ] ; } }
void saxpy_cpu ( float * vecY , float * vecX , float alpha , int n ) { int i ; for ( i = 0 ; i < n ; i ++ ) vecY [ i ] = alpha * vecX [ i ] + vecY [ i ] ; } ||| __global__ void saxpy_gpu ( float * vecY , float * vecX , float alpha , int n ) { int x , y , i ; x = blockIdx . x * blockDim . x + threadIdx . x ; y = blockIdx . y * blockDim . y + threadIdx . y ; i = y * 1024 + x ; if ( i < n ) vecY [ i ] = alpha * vecX [ i ] + vecY [ i ] ; }
void set_valid_mask_cpu ( const float * score , float score_thr , int * valid_mask , int dims ) { for ( int tid = 0 ; tid < dims ; tid ++ ) { if ( score [ tid ] > score_thr ) { valid_mask [ tid ] = 1 ; } else { valid_mask [ tid ] = 0 ; } } } ||| __global__ void set_valid_mask ( const float * score , float score_thr , int * valid_mask , int dims ) { int tid = blockIdx . x * blockDim . x + threadIdx . x ; if ( tid >= dims ) { return ; } if ( score [ tid ] > score_thr ) { valid_mask [ tid ] = 1 ; } else { valid_mask [ tid ] = 0 ; } }
void copy_swap ( float * f_in , float * f_target , const int L_x ) { int k_x ; for ( k_x = 0 ; k_x < L_x ; k_x ++ ) { float tempval = 0.f ; tempval = f_in [ k_x ] ; f_in [ k_x ] = f_target [ k_x ] ; f_target [ k_x ] = tempval ; } } ||| __global__ void copy_swap ( float * f_in , float * f_target , const int L_x ) { const int k_x = threadIdx . x + blockIdx . x * blockDim . x ; if ( k_x >= L_x ) { return ; } float tempval = 0.f ; tempval = f_in [ k_x ] ; f_in [ k_x ] = f_target [ k_x ] ; f_target [ k_x ] = tempval ; }
void sum_backward ( float * db , float * dout , int r , int c ) { for ( int j = 0 ; j < c ; j ++ ) { for ( int i = 0 ; i < r ; i ++ ) { db [ j ] += dout [ i * c + j ] ; } } } ||| __global__ void Kernel_Sum_backward_opt2 ( float * db , float * sum , int r_sum , int c ) { unsigned int j = blockDim . x * blockIdx . x + threadIdx . x ; if ( j >= c ) return ; float temp = 0 ; for ( int i = 0 ; i < r_sum ; i ++ ) { temp += sum [ i * c + j ] ; } db [ j ] = temp ; }
void is_repeat ( int N , int * device_input , int * device_output ) { int idx ; for ( idx = 0 ; idx < N ; idx ++ ) { device_output [ idx ] = 0 ; if ( idx + 1 < N && device_input [ idx ] == device_input [ idx + 1 ] ) device_output [ idx ] = 1 ; } } ||| __global__ void is_repeat ( int N , int * device_input , int * device_output ) { int idx = blockDim . x * blockIdx . x + threadIdx . x ; if ( idx < N ) { device_output [ idx ] = 0 ; if ( idx + 1 < N && device_input [ idx ] == device_input [ idx + 1 ] ) device_output [ idx ] = 1 ; } }
void kmeans_average ( int * means , int * counts , int BID , int DIM ) { int bid ; int tid ; for ( bid = 0 ; bid < BID ; bid ++ ) { for ( tid = 0 ; tid < DIM ; tid ++ ) { if ( counts [ bid ] == 0 ) means [ bid * DIM + tid ] = 0 ; else means [ bid * DIM + tid ] /= counts [ bid ] ; } } } ||| __global__ void kmeans_average ( int * means , int * counts ) { if ( counts [ blockIdx . x ] == 0 ) means [ blockIdx . x * blockDim . x + threadIdx . x ] = 0 ; else means [ blockIdx . x * blockDim . x + threadIdx . x ] /= counts [ blockIdx . x ] ; }
void matPerRowDivInplace_cpu ( double * mat , const double * alphas , int m , int n ) { for ( int index = 0 ; index < m * n ; index ++ ) { int i = index / n ; int j = index % n ; mat [ i * n + j ] /= ( alphas [ i ] + 10 * 3 ) ; } } ||| __global__ void matPerRowDivInplaceKernel ( double * mat , const double * alphas , int m , int n ) { int index = blockIdx . x * blockDim . x + threadIdx . x ; if ( index < m * n ) { int i = index / n ; int j = index % n ; mat [ i * n + j ] /= ( alphas [ i ] + 10 * 3 ) ; } }
void compute_new_means ( float * mx , float * my , const float * sx , const float * sy , const int * c , int size ) { int cluster = 0 ; const int count = max ( 1 , c [ cluster ] ) ; for ( cluster = 0 ; cluster < size ; cluster ++ ) { mx [ cluster ] = sx [ cluster ] / count ; my [ cluster ] = sy [ cluster ] / count ; } } ||| __global__ void compute_new_means ( float * mx , float * my , const float * sx , const float * sy , const int * c ) { const int cluster = threadIdx . x ; const int count = max ( 1 , c [ cluster ] ) ; mx [ cluster ] = sx [ cluster ] / count ; my [ cluster ] = sy [ cluster ] / count ; }
void copy_array_d2d ( double * * src , double * * dst , int m , int n ) { int i , j ; for ( i = 1 ; i < m + 1 ; i ++ ) for ( j = 1 ; j < n + 1 ; j ++ ) dst [ i ] [ j ] = src [ i ] [ j ] ; } ||| __global__ void copy_array_d2d ( double * * src , double * * dst , int m , int n ) { int i , j ; i = blockIdx . x * blockDim . x + threadIdx . x ; j = blockIdx . y * blockDim . y + threadIdx . y ; if ( i >= 1 && i < m + 1 && j >= 1 && j < n + 1 ) dst [ i ] [ j ] = src [ i ] [ j ] ; }
void InitCCL ( int labelList [ ] , int reference [ ] , int width , int height ) { int x ; int y ; for ( x = 0 ; x < width ; x ++ ) { for ( y = 0 ; y < height ; y ++ ) { int id = x + y * width ; labelList [ id ] = reference [ id ] = id ; } } } ||| __global__ void InitCCL ( int labelList [ ] , int reference [ ] , int width , int height ) { int x = blockIdx . x * blockDim . x + threadIdx . x ; int y = blockIdx . y * blockDim . y + threadIdx . y ; if ( x >= width || y >= height ) return ; int id = x + y * width ; labelList [ id ] = reference [ id ] = id ; }
void cpu_set_sg ( int * sxz , int sxbeg , int szbeg , int jsx , int jsz , int ns , int npml , int nnz ) { for ( int id = 0 ; id < ns ; id ++ ) { sxz [ id ] = nnz * ( sxbeg + id * jsx + npml ) + ( szbeg + id * jsz + npml ) ; } } ||| __global__ void cuda_set_sg ( int * sxz , int sxbeg , int szbeg , int jsx , int jsz , int ns , int npml , int nnz ) { int id = threadIdx . x + blockDim . x * blockIdx . x ; if ( id < ns ) sxz [ id ] = nnz * ( sxbeg + id * jsx + npml ) + ( szbeg + id * jsz + npml ) ; }
void addMatrix ( float * a , float * b , float * c , int N ) { int i , j , idx ; for ( i = 0 ; i < N ; i ++ ) for ( j = 0 ; j < N ; j ++ ) { idx = i * N + j ; a [ idx ] = b [ idx ] + c [ idx ] ; } } ||| __global__ void addMatrixGPU ( float * a , float * b , float * c , int N ) { int idx ; int j = threadIdx . x + blockIdx . x * blockDim . x ; int i = threadIdx . y + blockIdx . y * blockDim . y ; if ( ( i < N ) && ( j < N ) ) { idx = i * N + j ; a [ idx ] = b [ idx ] + c [ idx ] ; } }
void resizedClsScore_cpu ( const float * score , const float * score_factors , float * output , int dims ) { for ( int tid = 0 ; tid < dims ; tid ++ ) { if ( score [ tid ] == ( -1 ) ) { output [ tid ] = -1 ; } else { output [ tid ] = score [ tid ] * score_factors [ tid ] ; } } } ||| __global__ void resizedClsScore ( const float * score , const float * score_factors , float * output , int dims ) { int tid = blockIdx . x * blockDim . x + threadIdx . x ; if ( tid >= dims ) { return ; } if ( score [ tid ] == ( -1 ) ) { output [ tid ] = -1 ; } else { output [ tid ] = score [ tid ] * score_factors [ tid ] ; } }
void l1_cpu ( int n , float * pred , float * truth , float * delta , float * error ) { int i ; for ( i = 0 ; i < n ; ++ i ) { float diff = truth [ i ] - pred [ i ] ; error [ i ] = fabs ( diff ) ; delta [ i ] = diff > 0 ? 1 : -1 ; } } ||| __global__ void l1_kernel ( int n , float * pred , float * truth , float * delta , float * error ) { int i = ( blockIdx . x + blockIdx . y * gridDim . x ) * blockDim . x + threadIdx . x ; if ( i < n ) { float diff = truth [ i ] - pred [ i ] ; error [ i ] = abs ( diff ) ; delta [ i ] = ( diff > 0 ) ? 1 : -1 ; } }
void AddMatrixOnCPU ( int * A , int * B , int * C , int nx , int ny ) { int i , j ; int cnt = 0 ; for ( j = 0 ; j < ny ; j ++ ) { for ( i = 0 ; i < nx ; i ++ ) { C [ cnt ] = A [ cnt ] + B [ cnt ] ; cnt ++ ; } } } ||| __global__ void AddMatrixOnGPU ( float * A , float * B , float * C , int nx , int ny ) { int i = threadIdx . x + blockIdx . x * blockDim . x ; int j = threadIdx . y + blockIdx . y * blockDim . y ; int idx = i * nx + j ; if ( i <= nx && j <= ny ) { C [ idx ] = A [ idx ] + B [ idx ] ; } }
void LreluForward ( float * srcData , float * dstData , int data_size , float alpha ) { for ( int i = 0 ; i < data_size ; i ++ ) { dstData [ i ] = srcData [ i ] > 0 ? srcData [ i ] : srcData [ i ] * alpha ; } } ||| __global__ void LreluForward ( float * srcData , float * dstData , int data_size , float alpha ) { int thread_index = threadIdx . x + blockIdx . x * blockDim . x ; int num_threads = blockDim . x * gridDim . x ; for ( int i = 0 ; i < data_size ; i += num_threads ) { int index = i + thread_index ; if ( index < data_size ) { dstData [ index ] = srcData [ index ] > 0 ? srcData [ index ] : srcData [ index ] * alpha ; } } }
void filterFFT_cpu ( float * FFT , float * filter , int nxprj2 , int nviews , float scale ) { for ( int i = 0 ; i < nviews ; i ++ ) { for ( int j = 0 ; i < nxprj2 ; j ++ ) { FFT [ i * nxprj2 + j ] *= filter [ i * nxprj2 + j ] * scale ; } } } ||| __global__ void filterFFT ( float * FFT , float * filter , int nxprj2 , int nviews , float scale ) { int j = blockIdx . x * blockDim . x + threadIdx . x ; int i = blockIdx . y * blockDim . y + threadIdx . y ; if ( i < nviews && j < nxprj2 ) FFT [ i * nxprj2 + j ] *= filter [ i * nxprj2 + j ] * scale ; }
void convertFloatToRGBA_cpu ( char * out_image , const float * in_image , int width , int height ) { for ( int x = 0 ; x < width ; x ++ ) { for ( int y = 0 ; y < height ; y ++ ) { char temp ; int IND = y * width + x ; float val = in_image [ IND ] ; temp = 255 ; out_image [ IND ] = temp ; } } } ||| __global__ void convertFloatToRGBA_kernel ( char * out_image , const float * in_image , int width , int height ) { const int x = blockIdx . x * blockDim . x + threadIdx . x ; const int y = blockIdx . y * blockDim . y + threadIdx . y ; char temp ; if ( x < width && y < height ) { int IND = y * width + x ; float val = in_image [ IND ] ; temp = 255 ; out_image [ IND ] = temp ; } }
void convertEdgeMaskToFloatCpu ( float * d_output , unsigned char * d_input , unsigned int width , unsigned int height ) { for ( int x = 0 ; x < width ; x ++ ) { for ( int y = 0 ; y < height ; y ++ ) { d_output [ y * width + x ] = min ( d_input [ y * width + x ] , d_input [ width * height + y * width + x ] ) ; } } } ||| __global__ void convertEdgeMaskToFloatDevice ( float * d_output , unsigned char * d_input , unsigned int width , unsigned int height ) { const int x = blockIdx . x * blockDim . x + threadIdx . x ; const int y = blockIdx . y * blockDim . y + threadIdx . y ; if ( x >= width || y >= height ) return ; d_output [ y * width + x ] = min ( d_input [ y * width + x ] , d_input [ width * height + y * width + x ] ) ; }
void gpu_matrix_transpose ( int * mat_in , int * mat_out , unsigned int rows , unsigned int cols ) { unsigned int idx ; unsigned int idy ; for ( idx = 0 ; idx < cols ; idx ++ ) { for ( idy = 0 ; idy < rows ; idy ++ ) { unsigned int pos = idy * cols + idx ; unsigned int trans_pos = idx * rows + idy ; mat_out [ trans_pos ] = mat_in [ pos ] ; } } } ||| __global__ void gpu_matrix_transpose ( int * mat_in , int * mat_out , unsigned int rows , unsigned int cols ) { unsigned int idx = blockIdx . x * blockDim . x + threadIdx . x ; unsigned int idy = blockIdx . y * blockDim . y + threadIdx . y ; if ( idx < cols && idy < rows ) { unsigned int pos = idy * cols + idx ; unsigned int trans_pos = idx * rows + idy ; mat_out [ trans_pos ] = mat_in [ pos ] ; } }
void LreluBackward ( float * srcDiff , float * dstDiff , float * srcData , int data_size , float alpha ) { for ( int i = 0 ; i < data_size ; i ++ ) { dstDiff [ i ] = ( srcData [ i ] > 0 ) ? srcDiff [ i ] * 1.0 : srcDiff [ i ] * alpha ; } } ||| __global__ void LreluBackward ( float * srcDiff , float * dstDiff , float * srcData , int data_size , float alpha ) { int thread_index = threadIdx . x + blockIdx . x * blockDim . x ; int num_threads = blockDim . x * gridDim . x ; for ( int i = 0 ; i < data_size ; i += num_threads ) { int index = i + thread_index ; if ( index < data_size ) { dstDiff [ index ] = srcDiff [ index ] * ( ( srcData [ index ] > 0 ) + ( srcData [ index ] <= 0 ) * alpha ) ; } } }
int cpuReduce ( int * N , const int size ) { if ( size == 1 ) return N [ 0 ] ; int stride = size / 2 ; for ( int i = 0 ; i < stride ; i ++ ) N [ i ] += N [ i + stride ] ; return cpuReduce ( N , stride ) ; } ||| __global__ void gpuReduceRecursive ( int * I , int * O , unsigned int n ) { unsigned int tid = threadIdx . x ; unsigned int idx = threadIdx . x + blockIdx . x * blockDim . x ; if ( idx >= n ) return ; int * N = I + blockIdx . x * blockDim . x ; for ( int stride = 1 ; stride < blockDim . x ; stride *= 2 ) { if ( ( tid % ( 2 * stride ) ) == 0 ) N [ tid ] += N [ tid + stride ] ; __syncthreads ( ) ; } if ( tid == 0 ) O [ blockIdx . x ] = N [ 0 ] ; }
void devidecountInnerCPU ( long Xsize , long Ysize , long Zsize , double * p , double * pn , int * pcountinner ) { for ( int tid = 0 ; tid < Xsize * Ysize * Zsize ; tid ++ ) { if ( pcountinner [ tid ] > 1 ) { p [ tid ] = pn [ tid ] / pcountinner [ tid ] ; pn [ tid ] = 0 ; } } } ||| __global__ void devidecountInner ( long Xsize , long Ysize , long Zsize , double * p , double * pn , int * pcountinner ) { long tid = threadIdx . x + blockDim . x * blockIdx . x ; while ( tid < Xsize * Ysize * Zsize ) { if ( pcountinner [ tid ] > 1 ) { p [ tid ] = pn [ tid ] / pcountinner [ tid ] ; pn [ tid ] = 0 ; } tid += blockDim . x * gridDim . x ; } }
void cpuConvertToBits ( int * bit_decisions , unsigned short * bit_stream , int dec_size ) { for ( int dec_index = 0 ; dec_index < dec_size ; dec_index ++ ) { int bit_index = dec_index * 2 ; int curr_decision = bit_decisions [ dec_index ] ; bit_stream [ bit_index ] = ( ( curr_decision & 2 ) >> 1 ) ; bit_stream [ bit_index + 1 ] = ( curr_decision & 1 ) ; } } ||| __global__ void cudaConvertToBits ( int * bit_decisions , unsigned short * bit_stream , int dec_size ) { int dec_index = ( blockIdx . x * blockDim . x ) + threadIdx . x ; int bit_index = dec_index * 2 ; if ( dec_index >= dec_size ) return ; int curr_decision = bit_decisions [ dec_index ] ; bit_stream [ bit_index ] = ( ( curr_decision & 2 ) >> 1 ) ; bit_stream [ bit_index + 1 ] = ( curr_decision & 1 ) ; }
void copyAliasRow ( int * devMat , int memWidth , int memHeight , int size ) { for ( int devMatX = 0 ; devMat < size ; devMat ++ ) { devMat [ memWidth * 0 + devMatX ] = devMat [ memWidth * ( memHeight - 2 ) + devMatX ] ; devMat [ memWidth * ( memHeight - 1 ) + devMatX ] = devMat [ memWidth * 1 + devMatX ] ; } } ||| __global__ void copyAliasRow ( int * devMat , int memWidth , int memHeight ) { int devMatX = blockIdx . x * blockDim . x + threadIdx . x + 1 ; devMat [ memWidth * 0 + devMatX ] = devMat [ memWidth * ( memHeight - 2 ) + devMatX ] ; devMat [ memWidth * ( memHeight - 1 ) + devMatX ] = devMat [ memWidth * 1 + devMatX ] ; }
double * ObjFeatures_circularity ( const int compCount , const int * areaRes , const double * perimeter ) { if ( compCount > 0 ) { double * circ = ( double * ) malloc ( compCount * sizeof ( double ) ) ; for ( int i = 0 ; i < compCount ; i ++ ) { circ [ i ] = ( 4.0 * 3.14159265359 * ( double ) areaRes [ i ] ) / ( perimeter [ i ] * perimeter [ i ] ) ; } return circ ; } return ( double * ) 0 ; } ||| __global__ void circularity ( const int compCount , const int * areaRes , const float * perimeterRes , float * circ ) { int tid = blockDim . x * blockIdx . x + threadIdx . x ; if ( tid < compCount ) { circ [ tid ] = ( 4.0 * 3.14159265359 * ( float ) areaRes [ tid ] ) / ( perimeterRes [ tid ] * perimeterRes [ tid ] ) ; } }
void devidecountCPU ( long Xsize , long Ysize , long Zsize , double * pint , int * pcount ) { int n = Xsize * Ysize * 2 + ( Ysize - 2 ) * Zsize * 2 + ( Xsize - 2 ) * ( Zsize - 2 ) * 2 ; for ( int tid = 0 ; tid < n * n ; tid ++ ) { if ( pcount [ tid ] > 1 ) { pint [ tid ] /= pcount [ tid ] ; } } } ||| __global__ void devidecount ( long Xsize , long Ysize , long Zsize , double * pint , int * pcount ) { int n = Xsize * Ysize * 2 + ( Ysize - 2 ) * Zsize * 2 + ( Xsize - 2 ) * ( Zsize - 2 ) * 2 ; long tid = threadIdx . x + blockDim . x * blockIdx . x ; while ( tid < n * n ) { if ( pcount [ tid ] > 1 ) { pint [ tid ] /= pcount [ tid ] ; } tid += blockDim . x * gridDim . x ; } }
void bubbleSort ( int * p , const int size ) { for ( int i = 0 ; i < size - 1 ; i ++ ) { for ( int j = 0 ; j < size - i - 1 ; j ++ ) { if ( p [ j ] > p [ j + 1 ] ) { int temp = p [ j ] ; p [ j ] = p [ j + 1 ] ; p [ j + 1 ] = temp ; } } } } ||| __global__ void oddevenSort ( int * d_in , int size , int oe_flag , int & d_ch_flag ) { int idx = threadIdx . x + blockIdx . x * blockDim . x ; int p = 2 * idx + oe_flag ; if ( p + 1 < size ) { if ( d_in [ p ] > d_in [ p + 1 ] ) { int temp = d_in [ p ] ; d_in [ p ] = d_in [ p + 1 ] ; d_in [ p + 1 ] = temp ; d_ch_flag = 1 ; } } }
void matmul ( int a [ ] [ 100 ] , int b [ ] [ 100 ] , int c [ ] [ 100 ] ) { for ( int i = 0 ; i < 100 ; i ++ ) { for ( int j = 0 ; j < 100 ; j ++ ) { c [ i ] [ j ] = 0 ; for ( int k = 0 ; k < 100 ; k ++ ) c [ i ] [ j ] += a [ i ] [ k ] * b [ k ] [ j ] ; } } } ||| __global__ void matmul ( float * a , float * b , float * c , int width ) { float result = 0 ; int row = blockIdx . y * blockDim . y + threadIdx . y ; int col = blockIdx . x * blockDim . x + threadIdx . x ; for ( int k = 0 ; k < width ; k ++ ) { result += a [ row * width + k ] * b [ k * width + col ] ; } c [ row * width + col ] = result ; }
void cudaKernel_estimateSnr_cpu ( const float * corrSum , const int * corrValidCount , const float * maxval , float * snrValue , const int size ) { for ( int idx = 0 ; idx < size ; idx ++ ) { float mean = ( corrSum [ idx ] - maxval [ idx ] * maxval [ idx ] ) / ( corrValidCount [ idx ] - 1 ) ; snrValue [ idx ] = maxval [ idx ] * maxval [ idx ] / mean ; } } ||| __global__ void cudaKernel_estimateSnr ( const float * corrSum , const int * corrValidCount , const float * maxval , float * snrValue , const int size ) { int idx = threadIdx . x + blockDim . x * blockIdx . x ; if ( idx >= size ) return ; float mean = ( corrSum [ idx ] - maxval [ idx ] * maxval [ idx ] ) / ( corrValidCount [ idx ] - 1 ) ; snrValue [ idx ] = maxval [ idx ] * maxval [ idx ] / mean ; }
void cpu_sgemm ( float * C , float * A , float * B , long size ) { for ( long i = 0 ; i < size ; i ++ ) { for ( long k = 0 ; k < size ; k ++ ) { for ( long j = 0 ; j < size ; j ++ ) { C [ i * size + j ] += A [ i * size + k ] * B [ k * size + j ] ; } } } } ||| __global__ void naive_sgemm_kernel ( float * C , float * A , float * B , long size ) { const long i = blockIdx . x * blockDim . x + threadIdx . x ; const long j = blockIdx . y * blockDim . y + threadIdx . y ; float val = 0.0 ; if ( i >= size || j >= size ) return ; for ( long k = 0 ; k < size ; k ++ ) { val += A [ i * size + k ] * B [ k * size + j ] ; } C [ i * size + j ] += val ; }
void kernelXor ( unsigned int key , char * input_str_cuda , unsigned char * possible_plaintext_str_cuda , int input_length ) { int id ; char * keyCharPtr ; for ( id = 0 ; id < input_length ; id ++ ) { int keyIndex = id % 4 ; keyCharPtr = ( ( char * ) & key ) ; char keyChar = keyCharPtr [ keyIndex ] ; possible_plaintext_str_cuda [ id ] = keyChar ^ input_str_cuda [ id ] ; } } ||| __global__ void kernelXor ( unsigned int key , char * input_str_cuda , unsigned char * possible_plaintext_str_cuda , int input_length ) { int id = threadIdx . x + blockDim . x * blockIdx . x ; if ( id >= input_length ) return ; int keyIndex = id % 4 ; char * keyCharPtr = ( ( char * ) & key ) ; char keyChar = keyCharPtr [ keyIndex ] ; possible_plaintext_str_cuda [ id ] = keyChar ^ input_str_cuda [ id ] ; }
void envejecer_kernel_cpu ( int * estado , int * edad , int * pupacion , int * N_mobil , int dia ) { int N = N_mobil [ 0 ] ; for ( int id = 0 ; id < N ; id ++ ) { if ( dia < 80 || dia > 320 ) { if ( edad [ id ] > pupacion [ id ] ) edad [ id ] ++ ; } else { edad [ id ] ++ ; } } } ||| __global__ void envejecer_kernel ( int * estado , int * edad , int * pupacion , int * N_mobil , int dia ) { int N = N_mobil [ 0 ] ; int id = blockIdx . x * blockDim . x + threadIdx . x ; if ( id < N ) { if ( dia < 80 || dia > 320 ) { if ( edad [ id ] > pupacion [ id ] ) edad [ id ] ++ ; } else { edad [ id ] ++ ; } } }
void globalCalculateKernel ( float * c , float * a , float * b , int size ) { int i ; int j ; for ( i = 0 ; i < size ; i ++ ) { for ( j = 0 ; j < size ; j ++ ) { c [ i * j ] = sin ( a [ i * j ] ) * sin ( a [ i * j ] ) + cos ( b [ i * j ] ) * cos ( b [ i * j ] ) * cos ( b [ i * j ] ) ; } } } ||| __global__ void globalCalculateKernel ( float * c , float * a , float * b ) { int i = blockIdx . x * blockDim . x + threadIdx . x ; int j = blockIdx . y * blockDim . y + threadIdx . y ; c [ i * j ] = sin ( a [ i * j ] ) * sin ( a [ i * j ] ) + cos ( b [ i * j ] ) * cos ( b [ i * j ] ) * cos ( b [ i * j ] ) ; }
void cpu_matrix_mul ( int * a , int * b , int * c , int N ) { for ( int row = 0 ; row < N ; row ++ ) { for ( int col = 0 ; col < N ; col ++ ) { int sum = 0 ; for ( int i = 0 ; i < N ; i ++ ) { sum += a [ row * N + i ] * b [ i * N + col ] ; } c [ row * N + col ] = sum ; } } } ||| __global__ void gpu_matrix_mul ( int * a , int * b , int * c , int N ) { int row = blockIdx . y * blockDim . y + threadIdx . y ; int col = blockIdx . x * blockDim . x + threadIdx . x ; int sum = 0 ; if ( col < N && row < N ) { for ( int i = 0 ; i < N ; i ++ ) { sum += a [ row * N + i ] * b [ i * N + col ] ; } c [ row * N + col ] = sum ; } }
void grayscale ( unsigned char * input , unsigned char * output , int size ) { unsigned char r , g , b ; for ( int i = 0 ; i < size ; i ++ ) { r = input [ 3 * i ] ; g = input [ 3 * i + 1 ] ; b = input [ 3 * i + 2 ] ; output [ i ] = ( unsigned char ) ( 0.21 * ( float ) r + 0.71 * ( float ) g + 0.07 * ( float ) b ) ; } } ||| __global__ void grayscale ( unsigned char * input , unsigned char * output , int size ) { unsigned char r , g , b ; int i = threadIdx . x + blockDim . x * blockIdx . x ; if ( i < size ) { r = input [ 3 * i ] ; g = input [ 3 * i + 1 ] ; b = input [ 3 * i + 2 ] ; output [ i ] = ( unsigned char ) ( 0.21 * ( float ) r + 0.71 * ( float ) g + 0.07 * ( float ) b ) ; } }
void subtractMean_cpu ( double * images , const double * meanImage , int imageNum , int pixelNum ) { for ( int col = 0 ; col < pixelNum ; col ++ ) { for ( int row = 0 ; row < imageNum ; ++ row ) { images [ row * pixelNum + col ] -= meanImage [ col ] ; if ( images [ row * pixelNum + col ] < 0.0 ) { images [ row * pixelNum + col ] = 0.0 ; } } } } ||| __global__ void subtractMean ( double * images , const double * meanImage , std :: size_t imageNum , std :: size_t pixelNum ) { std :: size_t col = blockIdx . x * blockDim . x + threadIdx . x ; if ( col >= pixelNum ) { return ; } for ( std :: size_t row = 0 ; row < imageNum ; ++ row ) { images [ row * pixelNum + col ] -= meanImage [ col ] ; if ( images [ row * pixelNum + col ] < 0.0 ) { images [ row * pixelNum + col ] = 0.0 ; } } }
void kernelMaximum ( float * maxhd , float * maxvd , int start , int size ) { int tx = start ; int max_hd = 1.175494351e-38F ; int max_vd = 1.175494351e-38F ; for ( ; tx < size ; tx ++ ) { if ( maxhd [ tx ] > max_hd ) max_hd = maxhd [ tx ] ; if ( maxvd [ tx ] > max_vd ) max_vd = maxvd [ tx ] ; } } ||| __global__ void kernelMaximum ( float * maxhd , float * maxvd , int start , int size ) { int tx = start + threadIdx . x ; for ( int i = size >> 1 ; i > 0 ; i >>= 1 ) { __syncthreads ( ) ; if ( tx < i ) { if ( maxhd [ tx ] < maxhd [ tx + i ] ) maxhd [ tx ] = maxhd [ tx + i ] ; if ( maxvd [ tx ] < maxvd [ tx + i ] ) maxvd [ tx ] = maxvd [ tx + i ] ; } ; } ; }
void SparseMatmul_forward ( float * a , float * b , float * c , int * indptr , int * indices , int p , int size ) { for ( int i = 0 ; i < size - 1 ; i ++ ) { for ( int jj = indptr [ i ] ; jj < indptr [ i + 1 ] ; jj ++ ) { int j = indices [ jj ] ; for ( int k = 0 ; k < p ; k ++ ) c [ i * p + k ] += a [ jj ] * b [ j * p + k ] ; } } } ||| __global__ void cuda_SparseMatmul_forward_kernel ( float * a_in , float * b_in , float * c_in , int * indptr , int * indices , int p ) { int i = blockIdx . x ; int k = threadIdx . x ; for ( int jj = indptr [ i ] ; jj < indptr [ i + 1 ] ; jj ++ ) { int j = indices [ jj ] ; c_in [ i * p + k ] += a_in [ jj ] * b_in [ j * p + k ] ; } }
void vectorMatrixMult ( long int totalPixels , int availablePixels , int outPixelOffset , float * matrix , float * vector , float * out ) { for ( long int i = 0 ; i < availablePixels ; i ++ ) { float sum = 0.0 ; for ( long int j = 0 ; j < totalPixels ; j ++ ) { sum += matrix [ i * totalPixels + j ] * vector [ j ] ; } out [ i + outPixelOffset ] = sum ; } } ||| __global__ void vectorMatrixMult ( long int totalPixels , int availablePixels , int outPixelOffset , float * matrix , float * vector , float * out ) { int index = blockIdx . x * blockDim . x + threadIdx . x ; int stride = blockDim . x * gridDim . x ; for ( long int i = index ; i < availablePixels ; i += stride ) { float sum = 0.0 ; for ( long int j = 0 ; j < totalPixels ; j ++ ) { sum += matrix [ i * totalPixels + j ] * vector [ j ] ; } out [ i + outPixelOffset ] = sum ; } }
void convertKinectDisparityInPlace_cpu ( float * d_disparity , int pitch , int width , int height , float depth_scale ) { for ( int x = 0 ; x < width ; x ++ ) { for ( int y = 0 ; y < height ; y ++ ) { float * d_in = ( float * ) ( ( char * ) d_disparity + y * pitch ) + x ; * d_in = ( * d_in == 0.0f ) ? 1 : ( - depth_scale / * d_in ) ; } } } ||| __global__ void convertKinectDisparityInPlace_kernel ( float * d_disparity , int pitch , int width , int height , float depth_scale ) { const int x = blockIdx . x * blockDim . x + threadIdx . x ; const int y = blockIdx . y * blockDim . y + threadIdx . y ; if ( ( x < width ) & ( y < height ) ) { float * d_in = ( float * ) ( ( char * ) d_disparity + y * pitch ) + x ; * d_in = ( * d_in == 0.0f ) ? 1 : ( - depth_scale / * d_in ) ; } }
void SparseMatmul_backward ( float * a , float * b_grad , float * c_grad , int * indptr , int * indices , int p , int size , float * grad ) { for ( int i = 0 ; i < size - 1 ; i ++ ) { for ( int jj = indptr [ i ] ; jj < indptr [ i + 1 ] ; jj ++ ) { int j = indices [ jj ] ; for ( int k = 0 ; k < p ; k ++ ) b_grad [ j * p + k ] += c_grad [ i * p + k ] * a [ jj ] ; } } } ||| __global__ void cuda_SparseMatmul_backward_kernel ( float * a_in , float * b_in , float * c_in , int * indptr , int * indices , int p ) { int i = blockIdx . x ; int k = threadIdx . x ; for ( int jj = indptr [ i ] ; jj < indptr [ i + 1 ] ; jj ++ ) { int j = indices [ jj ] ; b_in [ j * p + k ] += c_in [ i * p + k ] * a_in [ jj ] ; } }
void subsample_ind_and_labels_cpu ( int * d_ind_sub , const int * d_ind , unsigned int * d_label_sub , const unsigned int * d_label , int n_out , float inv_sub_factor ) { for ( int ind_out = 0 ; ind_out < n_out ; ind_out ++ ) { int ind_in = ( int ) floorf ( ( float ) ( ind_out ) * inv_sub_factor ) ; d_ind_sub [ ind_out ] = d_ind [ ind_in ] ; d_label_sub [ ind_out ] = d_label [ ind_in ] ; } } ||| __global__ void subsample_ind_and_labels_GPU ( int * d_ind_sub , const int * d_ind , unsigned int * d_label_sub , const unsigned int * d_label , int n_out , float inv_sub_factor ) { unsigned int ind_out = blockIdx . x * blockDim . x + threadIdx . x ; if ( ind_out < n_out ) { int ind_in = ( int ) floorf ( ( float ) ( ind_out ) * inv_sub_factor ) ; d_ind_sub [ ind_out ] = d_ind [ ind_in ] ; d_label_sub [ ind_out ] = d_label [ ind_in ] ; } }