void mxm_1d_cpu ( double * a , const int m , double * b , const int n , double * c , const int p ) { for ( int i = 0 ; i < m ; i ++ ) { for ( int k = 0 ; k < p ; k ++ ) { double s = 0.0 ; for ( int j = 0 ; j < n ; j ++ ) { s += a [ j * m + i ] * b [ k * n + j ] ; } c [ k * m + i ] = s ; } } } ||| __global__ void mxm_1d ( double * a , const int m , double * b , const int n , double * c , const int p ) { const int i = blockIdx . x * blockDim . x + threadIdx . x ; if ( i < m ) { for ( int k = 0 ; k < p ; k ++ ) { double s = 0.0 ; for ( int j = 0 ; j < n ; j ++ ) { s += a [ j * m + i ] * b [ k * n + j ] ; } c [ k * m + i ] = s ; } } }
void fabsf_clamp_cpu ( int N , float * X , int INCX , float clamp_min , float clamp_max ) { int i ; for ( i = 0 ; i < N ; ++ i ) { if ( X [ i * INCX ] >= 0 ) { X [ i * INCX ] = fmin ( clamp_max , fmax ( clamp_min , X [ i * INCX ] ) ) ; } else { X [ i * INCX ] = fmin ( - clamp_min , fmax ( - clamp_max , X [ i * INCX ] ) ) ; } } } ||| __global__ void fabsf_clamp_kernel ( int N , float * X , int INCX , float clamp_min , float clamp_max ) { int i = ( blockIdx . x + blockIdx . y * gridDim . x ) * blockDim . x + threadIdx . x ; if ( i < N ) { if ( X [ i * INCX ] >= 0 ) X [ i * INCX ] = fminf ( clamp_max , fmaxf ( clamp_min , X [ i * INCX ] ) ) ; else X [ i * INCX ] = fminf ( - clamp_min , fmaxf ( - clamp_max , X [ i * INCX ] ) ) ; } }
void cpu_matrix_mult ( int * h_a , int * h_b , int * h_result , int m , int n , int k ) { for ( int i = 0 ; i < m ; ++ i ) { for ( int j = 0 ; j < k ; ++ j ) { int tmp = 0.0 ; for ( int h = 0 ; h < n ; ++ h ) { tmp += h_a [ i * n + h ] * h_b [ h * k + j ] ; } h_result [ i * k + j ] = tmp ; } } } ||| __global__ void gpu_matrix_mult ( int * a , int * b , int * c , int m , int n , int k ) { int row = blockIdx . y * blockDim . y + threadIdx . y ; int col = blockIdx . x * blockDim . x + threadIdx . x ; int sum = 0 ; if ( col < k && row < m ) { for ( int i = 0 ; i < n ; i ++ ) { sum += a [ row * n + i ] * b [ i * k + col ] ; } c [ row * k + col ] = sum ; } }
inline void MulMatrixOnCPU ( float * A , float * B , float * C , int nx , int ny ) { int i , j , k ; float sum = 0.0 ; for ( i = 0 ; i < nx ; i ++ ) { for ( j = 0 ; j < ny ; j ++ ) { sum = 0.0 ; for ( k = 0 ; k < nx ; k ++ ) { sum = sum + A [ i * nx + k ] * B [ k * nx + j ] ; } C [ i * nx + j ] = sum ; } } } ||| __global__ void MulMatrixOnGPU ( float * A , float * B , float * C , int nx , int ny ) { int i = threadIdx . x + blockIdx . x * blockDim . x ; int j = threadIdx . y + blockIdx . y * blockDim . y ; int k ; if ( i < nx && j < ny ) { float sum = 0.0 ; for ( k = 0 ; k < nx ; k ++ ) { sum += A [ i * nx + k ] * B [ k * nx + j ] ; } C [ i * nx + j ] = sum ; } }
int matrixMulHost ( float * h_M , float * h_N , float * h_P , int width ) { int Pvalue ; for ( int row = 0 ; row < width ; ++ row ) { for ( int col = 0 ; col < width ; ++ col ) { Pvalue = 0 ; for ( int k = 0 ; k < width ; ++ k ) { Pvalue += h_M [ row * width + k ] * h_N [ k * width + col ] ; } h_P [ row * width + col ] = Pvalue ; } } return 0 ; } ||| __global__ void MatrixMulKernel ( float * d_M , float * d_N , float * d_P , int width ) { int Row = blockIdx . y * blockDim . y + threadIdx . y ; int Col = blockIdx . x * blockDim . x + threadIdx . x ; if ( ( Row < width ) && ( Col < width ) ) { float Pvalue = 0 ; for ( int i = 0 ; i < width ; ++ i ) { Pvalue += d_M [ Row * width + i ] * d_N [ i * width + Col ] ; } d_P [ Row * width + Col ] = Pvalue ; } }
void mmul_cpu ( const float * A , const float * B , float * C , int r1 , int c1 , int r2 , int c2 ) { for ( int idx = 0 ; idx < c2 ; idx ++ ) { for ( int idy = 0 ; idy < c1 ; idy ++ ) { float temp = 0 ; for ( int i = 0 ; i < c1 ; i ++ ) temp += A [ idy * c1 + i ] * B [ i * c2 + idx ] ; C [ idy * c2 + idx ] = temp ; } } } ||| __global__ void mmul ( const float * A , const float * B , float * C , int r1 , int c1 , int r2 , int c2 ) { int idx = threadIdx . x + blockDim . x * blockIdx . x ; int idy = threadIdx . y + blockDim . y * blockIdx . y ; if ( ( idx < c2 ) && ( idy < c1 ) ) { float temp = 0 ; for ( int i = 0 ; i < c1 ; i ++ ) temp += A [ idy * c1 + i ] * B [ i * c2 + idx ] ; C [ idy * c2 + idx ] = temp ; } }
void Dot ( float * C , float * A , float * B , const int r , const int c , const int n ) { float temp ; for ( int i = 0 ; i < r ; i ++ ) { for ( int j = 0 ; j < c ; j ++ ) { temp = 0.0 ; for ( int k = 0 ; k < n ; k ++ ) { temp += A [ i * n + k ] * B [ k * c + j ] ; } C [ i * c + j ] = temp ; } } } ||| __global__ void Kernel_Dot_reduction2 ( float * dev_c , float * reduction , int r , const int c , const int n , int size_block ) { unsigned int i = blockDim . x * blockIdx . x + threadIdx . x ; unsigned int j = blockDim . y * blockIdx . y + threadIdx . y ; if ( i >= r || j >= c ) return ; float temp = 0 ; for ( int k = 0 ; k < size_block ; k ++ ) { temp += reduction [ i * ( c * size_block ) + j * ( size_block ) + k ] ; } dev_c [ i * c + j ] = temp ; }
void Forwardsub_cpu ( double * RES , double * LS , double * LW , double * LPR , int NI , int NJ , int Start , int J , int n ) { for ( int i = 0 ; i < n ; i ++ ) { int IJ = ( ( Start + i ) * NI ) + ( J - ( Start + i ) ) ; RES [ IJ ] = ( RES [ IJ ] - LS [ IJ ] * RES [ IJ - 1 ] - LW [ IJ ] * RES [ IJ - NJ ] ) * LPR [ IJ ] ; } } ||| __global__ void Forwardsub ( double * RES , double * LS , double * LW , double * LPR , int NI , int NJ , int Start , int J , int n ) { int i = blockIdx . x * blockDim . x + threadIdx . x ; if ( i < n ) { int IJ = ( ( Start + i ) * NI ) + ( J - ( Start + i ) ) ; RES [ IJ ] = ( RES [ IJ ] - LS [ IJ ] * RES [ IJ - 1 ] - LW [ IJ ] * RES [ IJ - NJ ] ) * LPR [ IJ ] ; } }
void cpu_rows_dc_offset_remove_layer_kernel ( float * output , float * input , unsigned int width , unsigned height , unsigned int depth ) { for ( unsigned int channel = 0 ; channel < depth ; channel ++ ) for ( unsigned int row = 0 ; row < height ; row ++ ) for ( unsigned int column = 0 ; column < ( width - 1 ) ; column ++ ) { unsigned int idx = ( channel * height + row ) * width + column ; output [ idx ] = input [ idx ] - input [ idx + 1 ] ; } } ||| __global__ void cuda_rows_dc_offset_remove_layer_kernel ( float * output , float * input , unsigned int width , unsigned int height , unsigned int depth ) { unsigned int column = threadIdx . x + blockIdx . x * blockDim . x ; unsigned int row = threadIdx . y + blockIdx . y * blockDim . y ; unsigned int channel = threadIdx . z + blockIdx . z * blockDim . z ; if ( channel < depth ) if ( row < height ) if ( column < ( width - 1 ) ) { unsigned int idx = ( channel * height + row ) * width + column ; output [ idx ] = input [ idx ] - input [ idx + 1 ] ; } }
void cpu_cross_correlate ( float * Isg , float * Iss , float * sp , float * gp , int npml , int nnz , int nnx ) { for ( int i1 = npml ; i1 < nnz - npml ; i1 ++ ) { for ( int i2 = npml ; i2 < nnx - npml ; i2 ++ ) { int id = i1 + i2 * nnz ; float ps = sp [ id ] ; float pg = gp [ id ] ; Isg [ id ] += ps * pg ; Iss [ id ] += ps * ps ; } } } ||| __global__ void cuda_cross_correlate ( float * Isg , float * Iss , float * sp , float * gp , int npml , int nnz , int nnx ) { int i1 = threadIdx . x + blockDim . x * blockIdx . x ; int i2 = threadIdx . y + blockDim . y * blockIdx . y ; int id = i1 + i2 * nnz ; if ( i1 >= npml && i1 < nnz - npml && i2 >= npml && i2 < nnx - npml ) { float ps = sp [ id ] ; float pg = gp [ id ] ; Isg [ id ] += ps * pg ; Iss [ id ] += ps * ps ; } }
void colorConvert ( unsigned char * grayImage , unsigned char * colorImage , int rows , int columns ) { int column ; int row ; for ( column = 0 ; column < columns ; column ++ ) { for ( row = 0 ; row < rows ; row ++ ) { int offset = ( column ) + ( columns * row ) ; unsigned char grayValue = 0.07 * colorImage [ offset * 3 ] + 0.71 * colorImage [ offset * 3 + 1 ] + 0.21 * colorImage [ offset * 3 + 2 ] ; grayImage [ offset ] = grayValue ; } } } ||| __global__ void colorConvert ( unsigned char * grayImage , unsigned char * colorImage , int rows , int columns ) { int column = blockIdx . x * blockDim . x + threadIdx . x ; int row = blockIdx . y * blockDim . y + threadIdx . y ; if ( ( column < columns ) && ( row < rows ) ) { int offset = ( column ) + ( columns * row ) ; unsigned char grayValue = 0.07 * colorImage [ offset * 3 ] + 0.71 * colorImage [ offset * 3 + 1 ] + 0.21 * colorImage [ offset * 3 + 2 ] ; grayImage [ offset ] = grayValue ; } }
void init_image_array_CPU ( unsigned long long int * image , int pixels_per_image ) { for ( int my_pixel = 0 ; my_pixel < pixels_per_image ; my_pixel ++ ) { image [ my_pixel ] = ( unsigned long long int ) ( 0 ) ; my_pixel += pixels_per_image ; image [ my_pixel ] = ( unsigned long long int ) ( 0 ) ; my_pixel += pixels_per_image ; image [ my_pixel ] = ( unsigned long long int ) ( 0 ) ; my_pixel += pixels_per_image ; image [ my_pixel ] = ( unsigned long long int ) ( 0 ) ; } } ||| __global__ void init_image_array_GPU ( unsigned long long int * image , int pixels_per_image ) { int my_pixel = threadIdx . x + blockIdx . x * blockDim . x ; if ( my_pixel < pixels_per_image ) { image [ my_pixel ] = ( unsigned long long int ) ( 0 ) ; my_pixel += pixels_per_image ; image [ my_pixel ] = ( unsigned long long int ) ( 0 ) ; my_pixel += pixels_per_image ; image [ my_pixel ] = ( unsigned long long int ) ( 0 ) ; my_pixel += pixels_per_image ; image [ my_pixel ] = ( unsigned long long int ) ( 0 ) ; } }
void diffusion ( const double * x0 , double * x1 , int nx , int ny , double dt ) { int i , j ; auto width = nx + 2 ; for ( j = 1 ; j < ny + 1 ; ++ j ) { for ( i = 1 ; i < nx + 1 ; ++ i ) { auto pos = i + j * width ; x1 [ pos ] = x0 [ pos ] + dt * ( -4. * x0 [ pos ] + x0 [ pos - width ] + x0 [ pos + width ] + x0 [ pos - 1 ] + x0 [ pos + 1 ] ) ; } } } ||| __global__ void diffusion ( double * x0 , double * x1 , int nx , int ny , double dt ) { int i = threadIdx . x + blockDim . x * blockIdx . x + 1 ; int j = threadIdx . y + blockDim . y * blockIdx . y + 1 ; if ( i < nx - 1 && j < ny - 1 ) { int pos = nx * j + i ; x1 [ pos ] = x0 [ pos ] + dt * ( -4. * x0 [ pos ] + x0 [ pos - 1 ] + x0 [ pos + 1 ] + x0 [ pos - nx ] + x0 [ pos + nx ] ) ; } }
void compute_b_minus_Rx ( double * out , double * x , double * b , double * cotans , int * neighbors , int meshStride , int n ) { for ( int i = 0 ; i < n ; i ++ ) { out [ i ] = b [ i ] ; for ( int iN = 0 ; iN < meshStride ; ++ iN ) { int neighbor = neighbors [ i * meshStride + iN ] ; double weight = cotans [ i * meshStride + iN ] ; out [ i ] += weight * x [ neighbor ] ; } } } ||| __global__ void compute_b_minus_Rx ( double * out , double * x , double * b , double * cotans , int * neighbors , int meshStride , int n ) { int index = blockIdx . x * blockDim . x + threadIdx . x ; int stride = gridDim . x * blockDim . x ; for ( int i = index ; i < n ; i += stride ) { out [ i ] = b [ i ] ; for ( int iN = 0 ; iN < meshStride ; ++ iN ) { int neighbor = neighbors [ i * meshStride + iN ] ; double weight = cotans [ i * meshStride + iN ] ; out [ i ] += weight * x [ neighbor ] ; } } }
void binarize_weights ( float * weights , int n , int size , float * binary ) { int i , f ; for ( f = 0 ; f < n ; ++ f ) { float mean = 0 ; for ( i = 0 ; i < size ; ++ i ) { mean += fabs ( weights [ f * size + i ] ) ; } mean = mean / size ; for ( i = 0 ; i < size ; ++ i ) { binary [ f * size + i ] = ( weights [ f * size + i ] > 0 ) ? mean : - mean ; } } } ||| __global__ void binarize_weights_kernel ( float * weights , int n , int size , float * binary ) { int f = ( blockIdx . x + blockIdx . y * gridDim . x ) * blockDim . x + threadIdx . x ; if ( f >= n ) return ; int i = 0 ; float mean = 0 ; for ( i = 0 ; i < size ; ++ i ) { mean += abs ( weights [ f * size + i ] ) ; } mean = mean / size ; for ( i = 0 ; i < size ; ++ i ) { binary [ f * size + i ] = ( weights [ f * size + i ] > 0 ) ? mean : - mean ; } }
void gather_points_kernel ( int b , int c , int n , int m , const float * points , const int * idx , float * out ) { for ( int i = 0 ; i < b ; i ++ ) { for ( int l = 0 ; l < c ; l ++ ) { for ( int j = 0 ; j < m ; j ++ ) { int a = idx [ i * m + j ] ; out [ ( i * c + l ) * m + j ] = points [ ( i * c + l ) * n + a ] ; } } } } ||| __global__ void gather_points_kernel ( int b , int c , int n , int m , const float * __restrict__ points , const int * __restrict__ idx , float * __restrict__ out ) { for ( int i = blockIdx . x ; i < b ; i += gridDim . x ) { for ( int l = blockIdx . y ; l < c ; l += gridDim . y ) { for ( int j = threadIdx . x ; j < m ; j += blockDim . x ) { int a = idx [ i * m + j ] ; out [ ( i * c + l ) * m + j ] = points [ ( i * c + l ) * n + a ] ; } } } }
void matrix_mult ( int left_rows , int shared_dimensions , int right_columns , float * left , float * right , float * result ) { int row ; int column ; int cell ; for ( row = 0 ; row < left_rows ; row ++ ) { for ( column = 0 ; column < right_columns ; column ++ ) { result [ row * right_columns + column ] = 0 ; for ( cell = 0 ; cell < shared_dimensions ; cell ++ ) { result [ row * right_columns + column ] += left [ row * shared_dimensions + cell ] * right [ cell * right_columns + column ] ; } } } } ||| __global__ void gpu_matrix_mult ( int left_rows , int shared_dimensions , int right_columns , float * left , float * right , float * result ) { int row = blockIdx . y * blockDim . y + threadIdx . y ; int column = blockIdx . x * blockDim . x + threadIdx . x ; if ( row < left_rows && column < right_columns ) { int index = row * right_columns + column ; result [ index ] = 0 ; int cell ; for ( cell = 0 ; cell < shared_dimensions ; cell ++ ) { result [ index ] += left [ row * shared_dimensions + cell ] * right [ cell * right_columns + column ] ; } } }
void matrixMultiplication_cpu ( int * host_a , int * host_b , int * host_c , int row_a , int col_a , int col_b ) { for ( int i = 0 ; i < row_a ; ++ i ) { for ( int j = 0 ; j < col_b ; ++ j ) { int tmp = 0 ; for ( int k = 0 ; k < col_a ; ++ k ) { tmp += host_a [ i * col_a + k ] * host_b [ j * col_b + k ] ; } host_c [ i * col_b + j ] = tmp ; } } } ||| __global__ void matrixMultiplication ( int * dev_a , int * dev_b , int * dev_c , int row_a , int col_a , int col_b ) { int row = threadIdx . y + blockIdx . y * blockDim . y ; int col = threadIdx . x + blockIdx . x * blockDim . x ; int ret = 0 ; if ( row < row_a && col < col_b ) { for ( int i = 0 ; i < col_a ; ++ i ) { ret += dev_a [ row * col_a + i ] * dev_b [ i * col_b + col ] ; } dev_c [ row * col_b + col ] = ret ; } }
void Backwardsub ( double * U , double * RES , double * UN , double * UE , double * LPR , int NI , int NJ , int End , int J , int n ) { for ( int i = 0 ; i < n ; i ++ ) { int IJ = ( ( End - i ) * NI ) + ( J - ( End - i ) ) ; RES [ IJ ] = RES [ IJ ] - UN [ IJ ] * RES [ IJ + 1 ] - UE [ IJ ] * RES [ IJ + NJ ] ; U [ IJ ] = U [ IJ ] + RES [ IJ ] ; } } ||| __global__ void Backwardsub ( double * U , double * RES , double * UN , double * UE , double * LPR , int NI , int NJ , int End , int J , int n ) { int i = blockIdx . x * blockDim . x + threadIdx . x ; if ( i < n ) { int IJ = ( ( End - i ) * NI ) + ( J - ( End - i ) ) ; RES [ IJ ] = RES [ IJ ] - UN [ IJ ] * RES [ IJ + 1 ] - UE [ IJ ] * RES [ IJ + NJ ] ; U [ IJ ] = U [ IJ ] + RES [ IJ ] ; } }
void convolution_cpu_1d ( float * input , const float * mask , float * output , int array_size , int mask_size ) { int MASK_RADIUS = mask_size / 2 ; float temp = 0.0f ; int ELEMENT_INDEX = 0 ; for ( int i = 0 ; i < array_size ; i ++ ) { temp = 0 ; for ( int j = 0 ; j < mask_size ; j ++ ) { ELEMENT_INDEX = i - MASK_RADIUS + j ; if ( ! ( ELEMENT_INDEX < 0 || ELEMENT_INDEX > ( array_size - 1 ) ) ) { temp += input [ ELEMENT_INDEX ] * mask [ j ] ; } } output [ i ] = temp ; } } ||| __global__ void convolution_gpu_1d_naive ( float * input , float * mask , float * output , int array_size , int mask_size ) { int gid = blockIdx . x * blockDim . x + threadIdx . x ; int MASK_RADIUS = mask_size / 2 ; int ELEMENT_INDEX = 0 ; float temp = 0.0f ; if ( gid < array_size ) { for ( int j = 0 ; j < mask_size ; j ++ ) { ELEMENT_INDEX = gid - MASK_RADIUS + j ; if ( ! ( ELEMENT_INDEX < 0 || ELEMENT_INDEX > ( array_size - 1 ) ) ) { temp += input [ ELEMENT_INDEX ] * mask [ j ] ; } } output [ gid ] = temp ; } }
void getRho ( const int numOfNucl , const double * psi , const double * occNo , double * rho , const char debug ) { * rho = 0 ; for ( int i = 0 ; i < numOfNucl ; ++ i ) * rho += occNo [ i ] * psi [ i ] * psi [ i ] ; if ( debug == 1 ) printf ( "  DEBUG ▁ print ▁ of ▁ RHO :  \n  ▁ RHO ▁ = ▁ % f  \n  This ▁ is ▁ the ▁ last ▁ line ( RHO ) .  \n   \n   " , * rho ) ; } ||| __global__ void getRho_cuda ( const double * psi , const double * occNo , double * rho ) { extern __shared__ double dcopy [ ] ; dcopy [ threadIdx . x ] = occNo [ threadIdx . x ] * psi [ threadIdx . x ] * psi [ threadIdx . x ] ; __syncthreads ( ) ; for ( int tc = blockDim . x , stepSize = 1 ; tc > 0 ; tc >>= 1 , stepSize <<= 1 ) { int pa = threadIdx . x * stepSize ; int pb = pa + stepSize ; if ( pb < blockDim . x ) { dcopy [ pa ] += dcopy [ pb ] ; } } if ( threadIdx . x == 0 ) { * rho = dcopy [ 0 ] ; } }
void colLog2SumExp2_cpu ( const double * mat , double * buf , int m , int n ) { for ( int j = 0 ; j < n ; j ++ ) { double maximum = mat [ j ] ; for ( int i = 1 ; i < m ; i ++ ) { if ( mat [ i * n + j ] > maximum ) { maximum = mat [ i * n + j ] ; } } double res = 0.0 ; for ( int i = 0 ; i < m ; i ++ ) { res += mat [ i * n + j ] - maximum ; } buf [ j ] = res + maximum ; } } ||| __global__ void colLog2SumExp2Kernel ( const double * mat , double * buf , int m , int n ) { int j = blockIdx . x * blockDim . x + threadIdx . x ; if ( j < n ) { double maximum = mat [ j ] ; for ( int i = 1 ; i < m ; i ++ ) { if ( mat [ i * n + j ] > maximum ) { maximum = mat [ i * n + j ] ; } } double res = 0.0 ; for ( int i = 0 ; i < m ; i ++ ) { res += mat [ i * n + j ] - maximum ; } buf [ j ] = res + maximum ; } }
void bitPrune_cpu ( unsigned char * out , float * in , int frontPrune , int outputlength , int inputLength , int n ) { for ( int i = 0 ; i < n ; i ++ ) { int batch = i / outputlength ; int indexInBatch = i % outputlength ; int batchInJump = batch * inputLength ; int indexOutBatch = i % outputlength ; int batchOutJump = batch * outputlength ; int frontJump = frontPrune ; out [ batchOutJump + indexOutBatch ] = ( char ) ( in [ batchInJump + frontJump + indexInBatch ] > 0 ) ; } } ||| __global__ void bitPrune ( unsigned char * out , float * in , int frontPrune , int outputlength , int inputLength , int n ) { int i = blockIdx . x * blockDim . x + threadIdx . x ; if ( i >= n ) return ; int batch = i / outputlength ; int indexInBatch = i % outputlength ; int batchInJump = batch * inputLength ; int indexOutBatch = i % outputlength ; int batchOutJump = batch * outputlength ; int frontJump = frontPrune ; out [ batchOutJump + indexOutBatch ] = ( char ) ( in [ batchInJump + frontJump + indexInBatch ] > 0 ) ; }
void residual ( double * out , double * x , double * b , double * cotans , int * neighbors , double * diag , int meshStride , int n ) { for ( int i = 0 ; i < n ; i ++ ) { out [ i ] = diag [ i ] * x [ i ] - b [ i ] ; for ( int iN = 0 ; iN < meshStride ; ++ iN ) { int neighbor = neighbors [ i * meshStride + iN ] ; double weight = cotans [ i * meshStride + iN ] ; out [ i ] -= weight * x [ neighbor ] ; } } } ||| __global__ void residual ( double * out , double * x , double * b , double * cotans , int * neighbors , double * diag , int meshStride , int n ) { int index = blockIdx . x * blockDim . x + threadIdx . x ; int stride = gridDim . x * blockDim . x ; for ( int i = index ; i < n ; i += stride ) { out [ i ] = diag [ i ] * x [ i ] - b [ i ] ; for ( int iN = 0 ; iN < meshStride ; ++ iN ) { int neighbor = neighbors [ i * meshStride + iN ] ; double weight = cotans [ i * meshStride + iN ] ; out [ i ] -= weight * x [ neighbor ] ; } } }
void forward_avgpool_layer ( int batch , int c , int h , int w , float * input , float * output ) { int b , i , k ; for ( b = 0 ; b < batch ; ++ b ) { for ( k = 0 ; k < c ; ++ k ) { int out_index = k + b * c ; output [ out_index ] = 0 ; for ( i = 0 ; i < h * w ; ++ i ) { int in_index = i + h * w * ( k + b * c ) ; output [ out_index ] += input [ in_index ] ; } output [ out_index ] /= h * w ; } } } ||| __global__ void forward_avgpool_layer_kernel ( int n , int w , int h , int c , float * input , float * output ) { int id = ( blockIdx . x + blockIdx . y * gridDim . x ) * blockDim . x + threadIdx . x ; if ( id >= n ) return ; int k = id % c ; id /= c ; int b = id ; int i ; int out_index = ( k + c * b ) ; output [ out_index ] = 0 ; for ( i = 0 ; i < w * h ; ++ i ) { int in_index = i + h * w * ( k + b * c ) ; output [ out_index ] += input [ in_index ] ; } output [ out_index ] /= w * h ; }
void convolutionColumnCPU ( float * h_Dst , float * h_Src , float * h_Filter , int imageW , int imageH , int filterR ) { int x , y , k ; for ( y = 0 ; y < imageH ; y ++ ) { for ( x = 0 ; x < imageW ; x ++ ) { float sum = 0 ; for ( k = - filterR ; k <= filterR ; k ++ ) { int d = y + k ; if ( d >= 0 && d < imageH ) { sum += h_Src [ d * imageW + x ] * h_Filter [ filterR - k ] ; } h_Dst [ y * imageW + x ] = sum ; } } } } ||| __global__ void kernel_columns ( const float * filter , const float * buffer , float * output , int imageW , int imageH , int filterR ) { int idx_x = threadIdx . x + blockDim . x * blockIdx . x ; int idx_y = threadIdx . y + blockDim . y * blockIdx . y ; int grid_width = gridDim . x * blockDim . x ; int idx = grid_width * idx_y + idx_x ; float sum = 0 ; int k ; for ( k = - filterR ; k <= filterR ; k ++ ) { int d = idx_y + k ; if ( d >= 0 && d < imageH ) { sum += buffer [ d * imageW + idx_x ] * filter [ filterR - k ] ; } } output [ idx ] = sum ; }
void matrMult ( float * A , float * B , float * C , int rowsA , int colsA , int colsB ) { for ( int i = 0 ; i < rowsA ; ++ i ) { for ( int j = 0 ; j < colsB ; ++ j ) { for ( int k = 0 ; k < colsA ; ++ k ) { C [ i * colsB + j ] += A [ i * colsA + k ] * B [ k * colsB + j ] ; } } } } ||| __global__ void gpuMatrMultD ( float * Ad , float * Bd , float * Cd , int rowsA , int colsA , int colsB ) { int bIndx = blockIdx . x ; int bIndy = blockIdx . y ; int tIndx = threadIdx . x ; int tIndy = threadIdx . y ; Cd [ ( blockDim . x * bIndx + tIndx ) * colsB + blockDim . y * bIndy + tIndy ] = 0 ; for ( int k = 0 ; k < colsA ; ++ k ) { Cd [ ( blockDim . x * bIndx + tIndx ) * colsB + blockDim . y * bIndy + tIndy ] += Ad [ ( blockDim . x * bIndx + tIndx ) * colsA + k ] * Bd [ k * colsB + blockDim . y * bIndy + tIndy ] ; } }